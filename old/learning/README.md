This folder contains code related to learning texture detectors. This include extracting patches, transforming to feature vectors, and training classifiers.


## Naming Convention ##

A classifier is uniquely determined by a setting.
A setting specifies (1) training set and (2) model.
The training set further depends on (a) feature extractor (b) the positive/negative patch selection strategy ( c) stain type and (d) the structure name.

Each classifier has its own folder under `$CLF_ROOTDIR`.


## Dataset Id ##

1:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`
- margin = [100,200,300,400,5,6,7,8,9,1000]
- Image type: `Nissl`
- 3000 per label = 1000 per label per stack x 3 stacks (MD585, MD589, MD594)

2:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`
- margin = [200,500]
- Image type: `Nissl`
- 300 per label = 100 per label per stack x 3 stacks (MD585, MD589, MD594)

3:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`
- margin = [200,500]
- Image type: `NTB`
- 1000 per label = 1000 per label per stack x 1 stacks (MD635)

4:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`
- margin = [200,500]
- Image type: `NTB`
- 100 per label = 100 per label per stack x 1 stacks (MD635)

5:
- Feature cell-based: largeOrientationHist, largeSizeHist, largeLargeLinkLenHist, largeSmallLinkLenHist.
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`
- margin = [500]
- Image type: `Nissl`
- 10 per label per section per stack x 3 stacks (MD585, MD589, MD594)

6:
- Feature cell-based: largeOrientationHist, largeSizeHist, largeLargeLinkLenHist, largeSmallLinkLenHist.
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`
- margin = [500]
- Image type: `Nissl`
- 3 per label per section per stack x 3 stacks (MD585, MD589, MD594)
Fewer instances than 5. Used as test set.

7:
- Feature set: ?
- Classes include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`; margin = [500]
- Image type: nissl
- 10 per class per section per stack (only one stack MD585)

8:
- Feature set: ?
- Classes include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`; margin = [500]
- Image type: nissl
- 10 per class per section per stack (only one stack MD589)

9:
- Feature set: ?
- Classes include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass`; margin = [500]
- Image type: nissl
- 10 per class per section per stack (only one stack MD594)

10:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass` where margin = [500]
- Image type: `Nissl`
- 1000 per class, 1 stack (MD585)

11:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass` where margin = [500]
- Image type: `Nissl`
- 1000 per class, 1 stack (MD589)

12:
- Feature generated by neural network `Sat16ClassFinetuned`
- Patch labels include: `X`, `X_surround_[margin]_noclass`, `X_surround_[margin]_Y`, `noclass` where margin = [500]
- Image type: `Nissl`
- 1000 per class, 1 stack (MD594)

===============================================================

20:
- Inception-BN
- Nissl
- margin 500 only
- 1000 per class, 1 stack (MD585)

21:
- Inception-BN
- Nissl
- margin 500 only
- 1000 per class, 1 stack (MD589)

22:
- Inception-BN
- Nissl
- margin 500 only
- 1000 per class, 1 stack (MD594)

## Model Id ##
1:
clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0,
                                 fit_intercept=True, intercept_scaling=1, class_weight=None,
                                 random_state=None, solver='liblinear', max_iter=100, multi_class='ovr',
                                 verbose=0, warm_start=False, n_jobs=1)

2:
clf = SVC(C=1.0, kernel='linear', degree=3, gamma='auto', coef0=0.0, shrinking=True,
                  probability=True, tol=0.001, cache_size=1000, max_iter=-1,
              decision_function_shape=None, random_state=None)

3:
sv_uncalibrated = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.0001,
                                C=1.0, multi_class='ovr',
                                fit_intercept=True, intercept_scaling=1, max_iter=100)
clf = CalibratedClassifierCV(sv_uncalibrated)

4:
http://xgboost.readthedocs.io/en/latest/python/python_api.html
clf = XGBClassifier(max_depth=3, learning_rate=0.2, n_estimators=200,
                            silent=False, objective='binary:logistic', nthread=-1, gamma=0,
                            min_child_weight=20, max_delta_step=0, subsample=.8,
                            colsample_bytree=.8, colsample_bylevel=1, reg_alpha=0, reg_lambda=1,
                            scale_pos_weight=1, base_score=0.5, seed=0, missing=None)

5:
http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html
clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.3, n_estimators=200,
                                         subsample=1., criterion='friedman_mse',
                                         min_samples_split=50, min_samples_leaf=20,
                                         min_weight_fraction_leaf=0.0, max_depth=3,
                                         min_impurity_split=1e-07, init=None, random_state=None,
                                         max_features=None, verbose=1, max_leaf_nodes=None,
                                         warm_start=False, presort='auto')

6:
clf = GradientBoostingClassifier(loss='deviance', learning_rate=0.3, n_estimators=100,
                                         subsample=1., criterion='friedman_mse',
                                         min_samples_split=50, min_samples_leaf=20,
                                         min_weight_fraction_leaf=0.0, max_depth=5,
                                         min_impurity_split=1e-07, init=None, random_state=None,
                                         max_features=None, verbose=1, max_leaf_nodes=None,
                                         warm_start=False, presort='auto')

## Classifier Id ##

1: negatives are surrounding patches that are not of other positive classes

model = 1
margin = 500
Training set = 1
train: 0.88, test acc: 0.84

2: negatives are surrounding patches (margin=500) that include other positive classes

model = 1
margin = 500
Training set = 1
train .87, test acc: 0.82

3: negatives are surrounding patches that are not of other positive classes

model = 2
Slow - train acc: 0.69, test acc: 0.68
margin = 500
Training set = 1

4: negatives are surrounding patches that are not of other positive classes

model = 3
train 0.75, test 0.74
margin = 500
Training set = 1

5: negatives are surrounding patches that are not of other positive classes

model = 4
margin = 500
Training set = 1

6: negatives are surrounding patches that are not of other positive classes
sklearn.ensemble.GradientBoostingClassifier, max_depth=3, n_estimators=200

acc: 0.96/0.85
model = 5
margin = 500
Training set = 1

7: negatives are surrounding patches that are not of other positive classes
sklearn.ensemble.GradientBoostingClassifier, max_depth=5, n_estimators=100

acc: 0.98/0.87
model = 6
margin = 500
Training set = 1

8: negatives are all outside patches, including other positive classes

model = 1
Training set = 1

9: negatives are all outside patches, including other positive classes, weighted according to distance to the center of
the structure of interest.
All surrounds are used as negative, other outside patches are randomly sampled to be negative.

model = 1
thresh = 1500
def diminishing(x):
   return np.exp(-(x-thresh)/3000.)
Training set = 1

10: same as 2, except training set uses NTB

model = 1
Training set = 3

11: same as 8, except training set uses NTB

model = 1
Training set = 3

12: use 2 for nissl patches, and use 10 for ntb patches.

=============================== Cell-based feature ============================

13: same as 2, except using cell-based features
training set = 5

14:
model = 1
margin = 500
Training set = 7+8
Negatives are surrounding regions that include other positive classes
train acc 0.667
test acc on dataset 9: 0.665

15:
model = 1
margin = 500
Training set = 8+9
Negatives are surrounding regions that include other positive classes
train acc 0.678
test acc on dataset 7: 0.654

16:
model = 2
margin = 500
Training set = 7+8
Negatives are surrounding regions that include other positive classes
train acc 0.675
test acc on dataset 9: ?

17:
model = 4
margin = 500
Training set = 7+8
Negatives are surrounding regions that include other positive classes
train acc 0.802
test acc on dataset 9: 0.714

18:
model = 4
margin = 500
Training set = 8+9
Negatives are surrounding regions that include other positive classes
train acc 0.807
test acc on dataset 7: 0.700

19:
model = 5
margin = 500
Training set = 7+8
Negatives are surrounding regions that include other positive classes
train acc 0.907
test acc on dataset 9: 0.733

20:
model = 5
margin = 500
Training set = 8+9
Negatives are surrounding regions that include other positive classes
train acc 0.910
test acc on dataset 7: 0.721

21:
model = 6
margin = 500
Training set = 7+8
Negatives are surrounding regions that include other positive classes
train acc 0.946
test acc on dataset 9: 0.741

22:
model = 6
margin = 500
Training set = 8+9
Negatives are surrounding regions that include other positive classes
train acc 0.95
test acc on dataset 7: 0.728

23:
model = 1
margin = 500
Training set = 10 + 11
Negatives are surrounding regions that include other positive classes
train acc: 0.887
test acc on dataset 12: 0.792

24:
model = 1
margin = 500
Training set = 11 + 12
Negatives are surrounding regions that include other positive classes
train acc: 0.891
test acc on dataset 10: 0.780

25:
model = 5
margin = 500
Training set = 10 + 11
Negatives are surrounding regions that include other positive classes
train acc: 0.965
test acc on dataset 12: 0.806

26:
model = 6
margin = 500
Training set = 10 + 11
Negatives are surrounding regions that include other positive classes
train acc: 0.986
test acc on dataset 12: 0.813

===============================

30:
train set: 20/21
neg_composition: neg_has_all_surround
margin: 500
model: lr
train acc: 0.86
test set: 22
test acc: 0.80

31:
train set: 21/22
neg_composition: neg_has_all_surround
margin: 500
model: lr
train acc: 0.863
test set: 20
test acc: 0.769

32:
train set: 20/22
neg_composition: neg_has_all_surround
margin: 500
model: lr
train acc: 0.869
test set: 21
test acc: 0.789

33:
train set: 20/21
neg_composition: neg_has_all_surround
margin: 500
model: gb1
train acc: 0.961
test set: 22
test acc: 0.813

34:
train set: 21/22
neg_composition: neg_has_all_surround
margin: 500
model: lr
train acc: 0.963
test set: 20
test acc: 0.778

35:
train set: 20/22
neg_composition: neg_has_all_surround
margin: 500
model: lr
train acc: 0.966
test set: 21
test acc: 0.798

## Evaluation ##

Q: Can we directly apply Nissl classifiers to NTB images?

setting = 10, Test set = 4: Test accuracy (X vs. surround) = 0.80
Setting = 11, Test set = 4: Test accuracy (X vs. surround) = 0.73

setting = 2, Test set = 4:  Test accuracy (X vs. surround) = 0.61
Setting = 8, Test set = 4: Test accuracy (X vs. surround) = 0.61

A: Nissl classifiers are clearly inferior to NTB classifiers when applied to NTB images.


## File Hierarchy ##

Classifiers are in `$CLF_ROOTDIR`.
Sparse scores are in `$SPARSE_SCORES_ROOTDIR`.
Dense score maps are in `$SCOREMAPS_ROOTDIR`.
Scoremap visualizations are in `$SCOREMAP_VIZ_ROOTDIR`.

### File Hierarchy for Classifiers ###
  - `datasets` contains the feature-extracted dataset. There are several datasets collected using different protocols.
    - Under the folder for each dataset are the features and addresses of different classes.
  - Other folders are classifiers trained with different settings.
    - `classifiers`: dump files of sklearn classifiers.
    - `eval`: evaluation figures, confusion matrices etc.

## Timing ##

Applying classifier: 2 min
Interpolate: 40 min
- each section (x 250): 120s
  - preprocess: 3s
  - evaluate spline (x 28): 5s for shrink=4; doubling shrink reduces time quadratically
  - upscale (x 28): 10s
  - save to hdf (x 28): 4s
Generate score map visualization: 13 min

## Feature Extraction Using Deep Neural Network ##

`extract_test_features_dnn.ipynb` runs on the Workstation equipped with GPU. It calls MXNet to compute features for patches through forward pass over a deep neural network.

To save computation time, only patches within foreground mask are extracted. Roughly 70k patches per section.
60s per section.
load saturation image: 51.70 seconds
extract, reshape, normalize: 114.39 seconds
predict: 288.41 seconds
save: 7.29 seconds
An entire image is roughly twice the mask area.


The generated features are place in `PATCH_FEATURES_ROOTDIR`. By default this is
`/media/yuncong/BstemAtlasData/CSHL_patch_features_Sat16ClassFinetuned_v2`.
The
`<stack>/<fn>_lossless_alignedTo_<anchor_fn>_cropped`. The features as n x 1024 array are stored in `<fn>_lossless_alignedTo_<anchor_fn>_cropped_features.hdf`. The locations of corresponding n patches are stored in `<fn>_lossless_alignedTo_<anchor_fn>_cropped_patch_locations.txt`.

This step takes 80 seconds per section (~25k patches).

## Train ##

Code for training classifiers is in `train_classifier_v2.ipynb`.
It works for both regular nissl and Neurotrace blue.

Store in `SVM_DIR`. By default this is `CSHL_patch_features_Sat16ClassFinetuned_v2_classifiers`. The classifiers are `<label>_svm.pkl`.

`Cluster executable`
`svm_v2.py`

Performance of trained classifiers can be visually analyzed in `test_classifier_performance.ipynb`.

We tried different versions of training dataset.
Testing uses inside vs. all surround

## Pipeline ##

The pipeline takes as input a stack whose DNN features are available.
It goes through the following stages:
- apply classifiers, generate score volumes
- global alignment with atlas
- local alignment with atlas

The pipeline can be controlled by `pipeline_features_to_scoremaps.ipynb`. It launches the proper
executable for each task on the computing cluster.

## Predict ##

Run `svm_v2.ipynb`

Load pre-trained classifiers. Apply every classifier to every feature file.

Predicted sparse scores are stored in `PREDICTIONS_ROOTDIR`.
By default this is `CSHL_patch_Sat16ClassFinetuned_v2_predictions`.

Under `<stack>/<fn>_lossless_alignedTo_<anchor_fn>_cropped`,
each sparse score file, as n x 1 array, is `<fn>_lossless_alignedTo_<anchor_fn>_cropped_<label>_sparseScores.hdf`

This step takes 900 seconds per stack.

## Interpolate ##

Run `interpolated_scoremaps_v2_distributed.ipynb`.

Script is `interpolated_scoremaps_v2.py`.
`interpolated_scoremaps_v2.py stack first_sec last_sec`.

Dense scoremaps are stored in `SCOREMAPS_ROOTDIR`. By default `CSHL_lossless_scoremaps_Sat16ClassFinetuned_v2`.
Under `<stack>/<fn>_lossless_alignedTo_<anchor_fn>_cropped`,
each dense score file is `<fn>_lossless_alignedTo_<anchor_fn>_cropped_<label>_denseScoreMap.hdf`,
and associated bounding box file `<fn>_lossless_alignedTo_<anchor_fn>_cropped_<label>_denseScoreMap_interpBox.txt`.

This step takes ? seconds per stack.

## Visualize ##

Run `visualize_scoremaps_v2_distributed.ipynb`.

Script is `visualize_scoremaps_v2.py`
`visualize_scoremaps_v2.py stack -b first_sec -e last_sec -a`

Score map visualizations are stored in `SCOREMAPVIZ_ROOTDIR`.
By default is `CSHL_scoremap_viz_Sat16ClassFinetuned_v2`.

Under `<label>/<stack>/<fn>_alignedTo_<anchor_fn>_scoremapViz_<label>.jpg`.

This step takes 500 seconds per stack.


## Evaluation / Analysis ##

The algorithm's goal is texture classification. The tasks are to separate textures inside a structure from textures outside the structure. Because we already have a strong location prior, we don't need strong texture score signal at places far from the structure's true location, so we take as negative examples only textures at the surrounding of the structure.

If annotations are available, we can compute the true positive, true negative, false positive and false negative rates of classifying each structure. We plot ACC as a function of the margin of the surrounding where negative samples are collected.
