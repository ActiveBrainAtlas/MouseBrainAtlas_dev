{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "reload(logging) # This is important for logging to work on workstation jupyter notebook.\n",
    "# See https://stackoverflow.com/questions/18786912/get-output-from-the-logging-module-in-ipython-notebook/21475297#21475297\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier_id = 71\n",
    "classifier_properties = classifier_settings.loc[classifier_id]\n",
    "\n",
    "margin = classifier_properties['margin']\n",
    "# model = classifier_properties['model']\n",
    "sample_weighting = classifier_properties['sample_weighting']\n",
    "neg_composition = classifier_properties['neg_composition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'inception-bn-blue-softmax'\n",
    "download_from_s3(os.path.join(MXNET_MODEL_ROOTDIR, model_dir_name), is_dir=True)\n",
    "model_name = 'inception-bn-blue-softmax'\n",
    "model_iteration = 0\n",
    "output_symbol_name = 'softmax_output'\n",
    "output_dim = 1024\n",
    "mean_img = np.load(os.path.join(MXNET_MODEL_ROOTDIR, model_dir_name, 'mean_224.npy'))\n",
    "\n",
    "# Reference on how to predict with mxnet model:\n",
    "# https://github.com/dmlc/mxnet-notebooks/blob/master/python/how_to/predict.ipynb\n",
    "model_prefix = os.path.join(MXNET_MODEL_ROOTDIR, model_dir_name, model_name)\n",
    "model0, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finetune\n",
    "# http://mxnet.io/how_to/finetune.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fine_tune_model(symbol, arg_params, num_classes, layer_name='flatten'):\n",
    "    \"\"\"\n",
    "    symbol: the pretrained network symbol\n",
    "    arg_params: the argument parameters of the pretrained model\n",
    "    num_classes: the number of classes for the fine-tune datasets\n",
    "    layer_name: the layer name before the last fully-connected layer\n",
    "    \"\"\"\n",
    "    all_layers = symbol.get_internals()\n",
    "    net = all_layers[layer_name + '_output']\n",
    "    net = mx.symbol.FullyConnected(data=net, num_hidden=num_classes, name='fc1')\n",
    "    net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n",
    "    new_args = {k: arg_params[k] for k in arg_params if 'fc1' not in k}\n",
    "    return (net, new_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "(new_sym, new_args) = get_fine_tune_model(model0, arg_params, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(symbol, arg_params, aux_params, train, val, batch_size, num_gpus, num_epoch, epoch_end_callback,\n",
    "       eval_end_callback):\n",
    "    devs = [mx.gpu(i) for i in range(num_gpus)]\n",
    "    mod = mx.mod.Module(symbol=symbol, context=devs)\n",
    "    mod.fit(train, val,\n",
    "        num_epoch=num_epoch,\n",
    "        arg_params=arg_params,\n",
    "        aux_params=aux_params,\n",
    "        allow_missing=True,\n",
    "        batch_end_callback = mx.callback.Speedometer(batch_size, 10),\n",
    "        kvstore='device',\n",
    "        optimizer='sgd',\n",
    "        optimizer_params={'learning_rate':0.01},\n",
    "        initializer=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2),\n",
    "        eval_metric='acc',\n",
    "           epoch_end_callback=epoch_end_callback,\n",
    "            eval_end_callback=eval_end_callback\n",
    "           )\n",
    "    metric = mx.metric.Accuracy()\n",
    "    return mod.score(val, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_per_gpu = 16\n",
    "num_gpus = 1\n",
    "batch_size = batch_per_gpu * num_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet.model import save_checkpoint\n",
    "\n",
    "def my_epoch_end_callback(prefix, period=1):\n",
    "    def _callback(epoch, sym, arg, aux):    \n",
    "        if epoch % period == 0:\n",
    "            save_checkpoint(prefix, epoch, sym, arg, aux)\n",
    "            symbol_fp = '%s-symbol.json' % prefix\n",
    "            param_fp = '%s-%04d.params' % (prefix, epoch)\n",
    "            upload_to_s3(symbol_fp)\n",
    "            upload_to_s3(param_fp)\n",
    "    return _callback\n",
    "\n",
    "def my_eval_end_callback(eval_metric_history):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        eval_metric_history (dict): {name: list of values}\n",
    "    \"\"\"\n",
    "    def _callback(param):\n",
    "        if not param.eval_metric:\n",
    "            return\n",
    "        name_value = param.eval_metric.get_name_value()\n",
    "        for name, value in name_value:\n",
    "            logging.info('Epoch[%d] Validation-%s=%f', param.epoch, name, value)\n",
    "            if name not in eval_metric_history:\n",
    "                eval_metric_history[name] = []\n",
    "            eval_metric_history[name].append(value)\n",
    "#             with open(acc_fp, 'a') as f:\n",
    "#                 f.write('Epoch[%d] Validation-%s=%f\\n' % (param.epoch, name, value))\n",
    "    return _callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for structure in structures_found:\n",
    "for structure in all_known_structures:\n",
    "# for structure in ['SC']:\n",
    "\n",
    "    try:\n",
    "\n",
    "        print structure\n",
    "\n",
    "        # Determine which labels to load.\n",
    "\n",
    "        structures_to_sample = [structure]\n",
    "        negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "\n",
    "        margins_to_sample = [margin] # (200: 100 um, 500: 250 um)\n",
    "        surround_positive_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix=surr_l) \n",
    "                                     for m in margins_to_sample\n",
    "                                     for s in structures_to_sample \n",
    "                                     for surr_l in all_known_structures\n",
    "                                     if surr_l != s]\n",
    "        surround_noclass_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix='noclass') \n",
    "                                     for m in margins_to_sample\n",
    "                                     for s in structures_to_sample]\n",
    "\n",
    "        if neg_composition == 'neg_has_everything_else':\n",
    "            labels_to_sample = structures_to_sample + negative_labels_to_sample\n",
    "        elif neg_composition == 'neg_has_only_surround_noclass':\n",
    "            labels_to_sample = structures_to_sample + surround_noclass_labels_to_sample + ['noclass']\n",
    "        elif neg_composition == 'neg_has_all_surround':\n",
    "            labels_to_sample = structures_to_sample + surround_positive_labels_to_sample + surround_noclass_labels_to_sample + ['noclass']\n",
    "\n",
    "        # labels_to_sample = ['Sp5C', 'Sp5C_surround_500_Sp5I', 'Sp5C_surround_500_noclass', 'Sp5C_surround_500_LRt']\n",
    "        # labels_to_sample = ['Sp5O', 'Sp5O_surround_500_Sp5I', 'Sp5O_surround_500_noclass']\n",
    "#         labels_to_sample = ['SC', 'SC_surround_500_IC', 'SC_surround_500_noclass']\n",
    "\n",
    "        # Load training dataset.\n",
    "\n",
    "        training_set_ids = map(int, str(classifier_properties['train_set_id']).split('/'))\n",
    "        training_features, training_addresses = load_dataset_images(training_set_ids, labels_to_sample=labels_to_sample)\n",
    "\n",
    "        # convert patches data shape to nx1x224x224\n",
    "        training_features = {s: (patches - mean_img)[:, None, :, :] for s, patches in training_features.iteritems()}\n",
    "\n",
    "        # check which labels are collected\n",
    "        labels_found = training_addresses.keys()\n",
    "        structures_found = set([convert_to_original_name(l) for l in labels_found \n",
    "                                if convert_to_original_name(l) in labels_found]) - {'noclass'}\n",
    "\n",
    "        # Load test dataset.\n",
    "\n",
    "#         test_set_ids = [62]\n",
    "#         test_features, test_addresses = load_dataset_images(test_set_ids, labels_to_sample=labels_to_sample)\n",
    "#         test_features = {s: (patches - mean_img)[:, None, :, :] for s, patches in test_features.iteritems()}\n",
    "\n",
    "        #############################################\n",
    "\n",
    "\n",
    "        if neg_composition == 'neg_has_only_surround_noclass':\n",
    "            neg_classes = [convert_to_surround_name(structure, margin=margin, suffix='noclass')]\n",
    "        elif neg_composition == 'neg_has_all_surround':\n",
    "            neg_classes = [convert_to_surround_name(structure, margin=margin, suffix='noclass')]\n",
    "            for surr_s in structures_found:\n",
    "                c = convert_to_surround_name(structure, margin=margin, suffix=surr_s)\n",
    "                if c in labels_found:\n",
    "                    neg_classes.append(c)\n",
    "        elif neg_composition == 'neg_has_everything_else':\n",
    "            neg_classes = [structure + '_negative']\n",
    "        else:\n",
    "            raise Exception('neg_composition %s is not recognized.' % neg_composition)\n",
    "\n",
    "        ###########################\n",
    "        ## Define Sample Weights ##\n",
    "        ###########################\n",
    "\n",
    "    #     if sample_weighting == 'weighted':\n",
    "    #         neg_distances = np.concatenate([distances_to_structures[neg_class][structure] for neg_class in neg_classes])\n",
    "\n",
    "    #         sample_weights_neg = np.ones((n_neg,))\n",
    "    #         sample_weights_neg[neg_distances > thresh] = diminishing(neg_distances[neg_distances > thresh])\n",
    "    #         sample_weights = np.r_[np.ones((n_pos,)), sample_weights_neg]\n",
    "    #     else:\n",
    "    #         sample_weights = None\n",
    "\n",
    "        ###########################################################################################\n",
    "\n",
    "        train_features_pos = training_features[structure]\n",
    "        n_pos = len(train_features_pos)\n",
    "\n",
    "        train_features_neg = np.concatenate([training_features[neg_class] for neg_class in neg_classes])\n",
    "        n_neg = len(train_features_neg)\n",
    "\n",
    "        train_data = np.concatenate([train_features_pos, train_features_neg])\n",
    "        # For cnn, labels must be 0/1 rather than +1/-1\n",
    "        train_labels = np.r_[np.ones((n_pos, )), np.zeros((n_neg, ))]\n",
    "\n",
    "        train_data_iter = mx.io.NDArrayIter(\n",
    "            data=train_data, \n",
    "            batch_size=batch_size,\n",
    "            label=train_labels,\n",
    "            shuffle=True)\n",
    "\n",
    "        #####################################\n",
    "\n",
    "#         test_features_pos = test_features[structure]\n",
    "#         n_pos = len(test_features_pos)\n",
    "\n",
    "#         test_features_neg = np.concatenate([test_features[neg_class] for neg_class in neg_classes \\\n",
    "#                                        if neg_class in test_features])\n",
    "#         n_neg = len(test_features_neg)\n",
    "\n",
    "#         test_data = np.concatenate([test_features_pos, test_features_neg])\n",
    "#         # For cnn, labels must be 0/1 rather than +1/-1\n",
    "#         test_labels = np.r_[np.ones((n_pos, )), np.zeros((n_neg, ))]\n",
    "\n",
    "#         test_data_iter = mx.io.NDArrayIter(\n",
    "#             data=test_data, \n",
    "#             batch_size=batch_size,\n",
    "#             label=test_labels,\n",
    "#             shuffle=True)\n",
    "\n",
    "        eval_metric_history = {}\n",
    "\n",
    "        t = time.time()\n",
    "        prefix = os.path.join(MXNET_MODEL_ROOTDIR, model_dir_name, model_name + '_' + structure)\n",
    "#         mod_score = fit(new_sym, new_args, aux_params, train_data_iter, test_data_iter, \n",
    "#                         batch_size, num_gpus, num_epoch=10, epoch_end_callback=my_epoch_end_callback(prefix, period=5))\n",
    "        mod_score = fit(new_sym, new_args, aux_params, train_data_iter, train_data_iter, \n",
    "                        batch_size, num_gpus, num_epoch=50, \n",
    "                        epoch_end_callback=my_epoch_end_callback(prefix, period=5),\n",
    "                       eval_end_callback=my_eval_end_callback(eval_metric_history))\n",
    "        sys.stderr.write('Fitting classifier: %.2f seconds\\n' % (time.time() - t))\n",
    "        \n",
    "        save_pickle( eval_metric_history, prefix+'_evalMetricHistory.pkl')\n",
    "        upload_to_s3(prefix+'_evalMetricHistory.pkl')\n",
    "\n",
    "    #     clf_fp = DataManager.get_classifier_filepath(classifier_id=classifier_id, structure=structure)\n",
    "    #     create_parent_dir_if_not_exists(clf_fp)\n",
    "    #     joblib.dump(clf, clf_fp)\n",
    "    #     upload_to_s3(clf_fp)\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(\"Skip %s: %s.\\n\" % (structure, str(e)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
