{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['GORDON_REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class ProposalType(Enum):\n",
    "    GLOBAL = 'global'\n",
    "    LOCAL = 'local'\n",
    "    FREEFORM = 'freeform'\n",
    "    ALGORITHM = 'algorithm'\n",
    "    \n",
    "class PolygonType(Enum):\n",
    "    CLOSED = 'closed'\n",
    "    OPEN = 'open'\n",
    "    TEXTURE = 'textured'\n",
    "    TEXTURE_WITH_CONTOUR = 'texture with contour'\n",
    "    DIRECTION = 'directionality'\n",
    "    \n",
    "from matplotlib.path import Path\n",
    "%matplotlib inline\n",
    "\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dms = dict([(sc, DataManager(stack='MD594', section=sc, segm_params_id='tSLIC200', load_mask=False)) \n",
    "            for sc in range(47, 185)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dm in dms.itervalues():\n",
    "    dm.load_multiple_results(['texHist', 'spCentroids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_examples = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_examples.pkl', 'r'))\n",
    "label_position = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_position.pkl', 'r'))\n",
    "label_polygon = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_polygon.pkl', 'r'))\n",
    "label_texture = pickle.load(open(os.environ['GORDON_RESULT_DIR']+'/database/label_texture.pkl', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d = dict([(sec, np.array(polygon.exterior.coords, np.int)) for sec, polygon in label_polygon['7N'].iteritems()])\n",
    "# pickle.dump(d, open('/tmp/boundarCoords_7N.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dict([(sec, np.array(polygon.exterior.coords, np.int)) for sec, polygon in label_polygon['Pn'].iteritems()])\n",
    "# pickle.dump(d, open('/tmp/boundarCoords_Pn.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grow_cluster_section(sec, *args, **kwargs):\n",
    "    return dms[sec].grow_cluster(*args, **kwargs)\n",
    "\n",
    "def grow_clusters_from_sps(sec, sps):\n",
    "    \n",
    "    expansion_clusters_tuples = Parallel(n_jobs=16)\\\n",
    "    (delayed(grow_cluster_section)(sec, s, verbose=False, all_history=False, seed_weight=0,\\\n",
    "                                   num_sp_percentage_limit=0.05, min_size=1, min_distance=2, threshold_abs=-0.1,\n",
    "                                   threshold_rel=0.02, peakedness_limit=.002, method='rc-mean',\n",
    "                                   seed_dist_lim = 0.2, inter_sp_dist_lim=1.) for s in sps)\n",
    "\n",
    "    all_seed_cluster_score_tuples = [(seed, cl, sig) for seed, peaks in enumerate(expansion_clusters_tuples) \n",
    "                                     for cl, sig in zip(*peaks)]\n",
    "    all_seeds, all_clusters, all_scores = zip(*all_seed_cluster_score_tuples)\n",
    "\n",
    "    all_clusters_unique_dict = {}\n",
    "    for i, cl in enumerate(all_clusters):\n",
    "        all_clusters_unique_dict[frozenset(cl)] = i\n",
    "\n",
    "    all_unique_cluster_indices = all_clusters_unique_dict.values()\n",
    "    all_unique_cluster_scores = [all_scores[i] for i in all_unique_cluster_indices]\n",
    "    all_unique_cluster_indices_sorted = [all_unique_cluster_indices[i] for i in np.argsort(all_unique_cluster_scores)[::-1]]\n",
    "\n",
    "    all_unique_tuples = [all_seed_cluster_score_tuples[i] for i in all_unique_cluster_indices_sorted]\n",
    "\n",
    "    return all_unique_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cluster_coherence_score(sec, cluster, verbose=False):\n",
    "    \n",
    "    if len(cluster) > 1:\n",
    "        cluster_avg = dms[sec].texton_hists[cluster].mean(axis=0)\n",
    "        ds = np.squeeze(chi2s([cluster_avg], dms[sec].texton_hists[list(cluster)]))\n",
    "        var = ds.mean()\n",
    "    else:\n",
    "        var = 0\n",
    "    \n",
    "    return var\n",
    "\n",
    "# def compute_cluster_significance_score(sec, *args, **kwargs):\n",
    "#     return dms[sec].compute_cluster_score(*args, **kwargs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coherence_limit = .25\n",
    "area_limit = 60000\n",
    "nonoverlapping_area_limit = 2.\n",
    "bg_texton = 3\n",
    "bg_texton_percentage = .2\n",
    "significance_limit = 0.05\n",
    "consensus_limit = -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_proposal_properties(seeded_proposals, sec):\n",
    "    \n",
    "    dm = dms[sec]\n",
    "    \n",
    "    dm.load_multiple_results(['texHist', 'spAreas', 'spCentroids'])\n",
    "\n",
    "    properties = []\n",
    "    for seed, sps, sig in seeded_proposals:\n",
    "        props = {'seed': seed, 'sps': sps, 'sig': sig}\n",
    "        props['coherence'] = compute_cluster_coherence_score(sec, sps)\n",
    "        props['hist'] = dm.texton_hists[sps].mean(axis=0)\n",
    "        nz_hist = props['hist']\n",
    "        props['entropy'] = np.nan_to_num(-np.sum(nz_hist*np.log(nz_hist)))\n",
    "        props['centroid'] = dm.sp_centroids[cl, ::-1].mean(axis=0)\n",
    "        props['area'] = dm.sp_areas[sps].sum()\n",
    "        \n",
    "        properties.append(props)\n",
    "        \n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sort_clusters(sec, all_unique_tuples, label, sec2):\n",
    "    \n",
    "    dm = dms[sec]\n",
    "    \n",
    "    seeds, clusters, sigs = zip(*all_unique_tuples)\n",
    "    \n",
    "#     props = compute_proposal_properties(all_unique_tuples, sec)  \n",
    "#     cluster_areas = map(attrgetter('area'), props)\n",
    "#     cluster_centers = map(attrgetter('centroid'), props)\n",
    "    \n",
    "    tex_dists = chi2s(label_texture[label], [dm.texton_hists[sps].mean(axis=0) for sps in clusters])\n",
    "    \n",
    "#     remaining_cluster_indices_sortedByTexture = [remaining_cluster_indices[j] for j in np.argsort(tex_dists)]\n",
    "      \n",
    "    polygons = [Polygon(dm.vertices_from_dedges(dm.find_boundary_dedges_ordered(cl))) \n",
    "                for cl in clusters]\n",
    "\n",
    "    polygon_overlaps = []\n",
    "    for p in polygons:\n",
    "        try:\n",
    "#             polygon_overlaps.append(label_polygon[label][sec2].intersection(p).area)\n",
    "            polygon_overlaps.append(float(label_polygon[label][sec2].intersection(p).area)/label_polygon[label][sec2].union(p).area)\n",
    "        except:\n",
    "#             print list(p.exterior.coords)\n",
    "            polygon_overlaps.append(0)\n",
    "    \n",
    "#     rank = np.argsort(.3*scores_to_vote(polygon_overlaps) + .7*scores_to_vote(-tex_dists))[::-1]\n",
    "    rank = np.argsort(.5*scores_to_vote(polygon_overlaps) + .5*scores_to_vote(-tex_dists))[::-1]\n",
    "\n",
    "    all_remaining_clusters_sorted = [clusters[i] for i in rank]\n",
    "\n",
    "#     remaining_cluster_indices_sortedByOverlap = [remaining_cluster_indices[j] for j in np.argsort(polygon_overlaps)[::-1]]\n",
    "    \n",
    "#     all_remaining_clusters_sortedByTexture = [all_unique_clusters[i] for i in remaining_cluster_indices_sortedByTexture]\n",
    "\n",
    "#     all_remaining_clusters_sortedByOverlap = [all_unique_clusters[i] for i in remaining_cluster_indices_sortedByOverlap]\n",
    "    \n",
    "#     return all_remaining_clusters_sortedByTexture\n",
    "#     return all_remaining_clusters_sortedByOverlap\n",
    "    return all_remaining_clusters_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "section = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ks = np.array(label_position[label].keys())\n",
    "ds = ks - section\n",
    "\n",
    "next_labeled_section = ks[ds >= 0][0]\n",
    "prev_labeled_section = ks[ds <= 0][0]\n",
    "\n",
    "print 'prev_labeled_section', prev_labeled_section, 'next_labeled_section', next_labeled_section\n",
    "\n",
    "if abs(section - prev_labeled_section) < abs(section - next_labeled_section):\n",
    "    v1,v2,s1,s2,c0 = label_position[label][prev_labeled_section]\n",
    "else:\n",
    "    v1,v2,s1,s2,c0 = label_position[label][next_labeled_section]\n",
    "\n",
    "angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "ell_vertices = cv2.ellipse2Poly(tuple(c0.astype(np.int)), (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, 10)\n",
    "\n",
    "sps = np.where([Path(ell_vertices).contains_point(s) for s in dms[section].sp_centroids[:,::-1]])[0]\n",
    "\n",
    "print '%d sps to look at\\n' % len(sps)\n",
    "\n",
    "all_unique_tuples = grow_clusters_from_sps(section, sps)\n",
    "\n",
    "if abs(section - prev_labeled_section) < abs(section - next_labeled_section):\n",
    "    clusters_sorted = sort_clusters(section, all_unique_tuples, label, prev_labeled_section)\n",
    "else:\n",
    "    clusters_sorted = sort_clusters(section, all_unique_tuples, label, next_labeled_section)\n",
    "\n",
    "dm = dms[section]\n",
    "\n",
    "cs = dm.sp_centroids[clusters_sorted[0]]\n",
    "tight_ymin, tight_xmin = cs.min(axis=0).astype(np.int) - 300\n",
    "\n",
    "close_up = img_as_ubyte(dm.visualize_cluster(clusters_sorted[0], tight=True))\n",
    "\n",
    "angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "cv2.ellipse(close_up, tuple(c0.astype(np.int)-(tight_xmin, tight_ymin)), \n",
    "        (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, (0,255,0), 10)\n",
    "\n",
    "plt.imshow(close_up);\n",
    "# imsave('/tmp/close_up%d_%s.jpg'%(section, label), close_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "section = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_area_ellipses = {}\n",
    "detected_clusters = {}\n",
    "\n",
    "for label in ['Pn', '7n', '7N']:\n",
    "\n",
    "    ks = np.array(sorted(label_position[label].keys()))\n",
    "    ds = ks - section\n",
    "\n",
    "    next_labeled_section = ks[ds >= 0][0]\n",
    "    prev_labeled_section = ks[ds <= 0][-1]\n",
    "\n",
    "    print 'prev_labeled_section', prev_labeled_section, 'next_labeled_section', next_labeled_section\n",
    "\n",
    "    if abs(section - prev_labeled_section) < abs(section - next_labeled_section):\n",
    "        v1,v2,s1,s2,c0 = label_position[label][prev_labeled_section]\n",
    "    else:\n",
    "        v1,v2,s1,s2,c0 = label_position[label][next_labeled_section]\n",
    "\n",
    "    search_area_ellipses[label] = (v1,v2,s1,s2,c0)\n",
    "        \n",
    "    angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "    ell_vertices = cv2.ellipse2Poly(tuple(c0.astype(np.int)), (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, 10)\n",
    "\n",
    "    sps = np.where([Path(ell_vertices).contains_point(s) for s in dms[section].sp_centroids[:,::-1]])[0]\n",
    "\n",
    "    print '%d sps to look at\\n' % len(sps)\n",
    "\n",
    "    all_unique_tuples = grow_clusters_from_sps(section, sps)\n",
    "    \n",
    "    if abs(section - prev_labeled_section) < abs(section - next_labeled_section):\n",
    "        clusters_sorted = sort_clusters(section, all_unique_tuples, label, prev_labeled_section)\n",
    "    else:\n",
    "        clusters_sorted = sort_clusters(section, all_unique_tuples, label, next_labeled_section)\n",
    "        \n",
    "    dm = dms[section]\n",
    "\n",
    "    close_up = img_as_ubyte(dm.visualize_cluster(clusters_sorted[0], tight=True))\n",
    "    imsave('/tmp/closeup_%04d_%s.jpg'%(section, label), close_up)\n",
    "\n",
    "    detected_clusters[label] = clusters_sorted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detected_cluster_edges = dict([(l, dm.find_boundary_dedges_ordered(cl)) for l, cl in detected_clusters.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "atlas = img_as_ubyte(dm.visualize_edge_sets(detected_cluster_edges.values(), bg='originalImage'))\n",
    "\n",
    "for label in ['Pn', '7n', '7N']:\n",
    "    v1,v2,s1,s2,c0 = search_area_ellipses[label]\n",
    "\n",
    "    angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "    cv2.ellipse(atlas, tuple(c0.astype(np.int)-(dm.xmin, dm.ymin)), \n",
    "                (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, (0,255,0), 10)\n",
    "    \n",
    "    cy, cx = dm.sp_centroids[detected_clusters[label]].mean(axis=0)\n",
    "    \n",
    "    cv2.putText(atlas, label, (int(cx)-dm.xmin, int(cy)-dm.ymin), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                            3, ((255,0,0)), 3)\n",
    "\n",
    "imsave('/tmp/labeled_%04d_allLandmarks.jpg'%section, atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(search_area_ellipses, open('/tmp/search_area_ellipses_%04d.pkl'%section, 'w'))\n",
    "pickle.dump(detected_clusters, open('/tmp/detected_clusters_%04d.pkl'%section, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for section in range(139, 144):\n",
    "\n",
    "#     dm = dms[section]\n",
    "\n",
    "# #     search_area_ellipses = pickle.load(open('/tmp/search_area_ellipses_%04d.pkl'%section, 'r'))\n",
    "#     detected_clusters = pickle.load(open('/tmp/detected_clusters_%04d.pkl'%section, 'r'))\n",
    "\n",
    "# #     for label, cl in detected_clusters.iteritems():\n",
    "    \n",
    "# #         edges = dm.find_boundary_dedges_ordered(cl)\n",
    "# #         close_up = img_as_ubyte(dm.visualize_edge_set(edges, tight=True, bg='originalImage', linewidth=20))\n",
    "# #         imsave('/tmp/closeup_%04d_%s.jpg'%(section, label), close_up)\n",
    "        \n",
    "        \n",
    "#     detected_cluster_edges = dict([(l, dm.find_boundary_dedges_ordered(cl)) \n",
    "#                                    for l, cl in detected_clusters.iteritems()])\n",
    "\n",
    "#     atlas = img_as_ubyte(dm.visualize_edge_sets(detected_cluster_edges.values(), bg='originalImage',\n",
    "#                                                linewidth=20))\n",
    "\n",
    "#     for label in ['Pn', '7n', '7N']:\n",
    "#         v1,v2,s1,s2,c0 = search_area_ellipses[label]\n",
    "#         angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "#         cv2.ellipse(atlas, tuple(c0.astype(np.int)-(dm.xmin, dm.ymin)), \n",
    "#                     (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, (0,255,0), 10)\n",
    "\n",
    "#         cy, cx = dm.sp_centroids[detected_clusters[label]].mean(axis=0)\n",
    "\n",
    "#         cv2.putText(atlas, label, (int(cx)-dm.xmin, int(cy)-dm.ymin), cv2.FONT_HERSHEY_DUPLEX,\n",
    "#                                 3, ((255,0,0)), 3)\n",
    "\n",
    "#     imsave('/tmp/labeled_%04d_allLandmarks.jpg'%section, atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 sps to look at\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TopologyException: Input geom 1 is invalid: Self-intersection at or near point 4847.4438789934975 1693.6632739609838 at 4847.4438789934975 1693.6632739609838\n",
      "69\n",
      "25\n",
      "198\n",
      "152\n",
      "153\n",
      "144\n",
      "24\n",
      "80\n",
      "140\n",
      "22\n",
      "137\n",
      "190\n",
      "23\n",
      "85\n",
      "188\n",
      "82\n",
      "222\n",
      "352\n",
      "264\n",
      "268\n",
      "312\n",
      "297\n",
      "348\n",
      "285\n",
      "298\n",
      "224\n",
      "229\n",
      "330\n",
      "255\n",
      "251\n",
      "322\n",
      "202\n",
      "355\n",
      "442\n",
      "282\n",
      "479\n",
      "421\n",
      "418\n",
      "481\n",
      "414\n",
      "401\n",
      "356\n",
      "393\n",
      "396\n",
      "461\n",
      "363\n",
      "440\n",
      "339\n",
      "463\n",
      "576\n",
      "425\n",
      "606\n",
      "544\n",
      "564\n",
      "658\n",
      "527\n",
      "510\n",
      "466\n",
      "580\n",
      "523\n",
      "599\n",
      "467\n",
      "539\n",
      "447\n",
      "604\n",
      "646\n",
      "534\n",
      "660\n",
      "727\n",
      "655\n",
      "711\n",
      "585\n",
      "656\n",
      "720\n",
      "ERROR:shapely.geos:TopologyException: Input geom 1 is invalid: Self-intersection at or near point 4847.4438789934975 1693.6632739609838 at 4847.4438789934975 1693.6632739609838\n",
      "Self-intersection at or near point 4847.4438789934975 1693.6632739609838\n",
      "WARNING:shapely.geos:Self-intersection at or near point 4847.4438789934975 1693.6632739609838\n",
      "TopologyException: Input geom 1 is invalid: Self-intersection at or near point 6011.3036830841111 3159.139042370155 at 6011.3036830841111 3159.139042370155\n",
      "ERROR:shapely.geos:TopologyException: Input geom 1 is invalid: Self-intersection at or near point 6011.3036830841111 3159.139042370155 at 6011.3036830841111 3159.139042370155\n",
      "Self-intersection at or near point 6011.3036830841111 3159.139042370155\n",
      "WARNING:shapely.geos:Self-intersection at or near point 6011.3036830841111 3159.139042370155\n",
      "TopologyException: Input geom 1 is invalid: Self-intersection at or near point 6011.3036830841111 3159.139042370155 at 6011.3036830841111 3159.139042370155\n",
      "ERROR:shapely.geos:TopologyException: Input geom 1 is invalid: Self-intersection at or near point 6011.3036830841111 3159.139042370155 at 6011.3036830841111 3159.139042370155\n",
      "Self-intersection at or near point 6011.3036830841111 3159.139042370155\n",
      "WARNING:shapely.geos:Self-intersection at or near point 6011.3036830841111 3159.139042370155\n",
      "TopologyException: Input geom 1 is invalid: Self-intersection at or near point 4847.4728937204982 1693.6471397291023 at 4847.4728937204982 1693.6471397291023\n",
      "ERROR:shapely.geos:TopologyException: Input geom 1 is invalid: Self-intersection at or near point 4847.4728937204982 1693.6471397291023 at 4847.4728937204982 1693.6471397291023\n",
      "Self-intersection at or near point 4847.4728937204982 1693.6471397291023\n",
      "WARNING:shapely.geos:Self-intersection at or near point 4847.4728937204982 1693.6471397291023\n",
      "TopologyException: Input geom 1 is invalid: Self-intersection at or near point 6271 1527.4883720930231 at 6271 1527.4883720930231\n",
      "ERROR:shapely.geos:TopologyException: Input geom 1 is invalid: Self-intersection at or near point 6271 1527.4883720930231 at 6271 1527.4883720930231\n",
      "Self-intersection at or near point 6271 1527.4883720930231\n",
      "WARNING:shapely.geos:Self-intersection at or near point 6271 1527.4883720930231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 terminate due to seed_dist exceeds threshold 0.20126599598\n",
      "69 terminate due to seed_dist exceeds threshold 0.204327844472\n",
      "264 terminate due to seed_dist exceeds threshold 0.2025313339\n",
      "80 terminate due to over-size\n",
      "144 terminate due to over-size\n",
      "140 terminate due to over-size\n",
      "188 terminate due to over-size\n",
      "190 terminate due to over-size\n",
      "24 terminate due to over-size\n",
      "25 terminate due to over-size\n",
      "255 terminate due to over-size\n",
      "268 terminate due to over-size\n",
      "393 terminate due to over-size\n",
      "467 terminate due to over-size\n",
      "544 terminate due to over-size\n",
      "585 terminate due to over-size\n",
      "447222534285418510523348442599606655727 terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      "646463720527656711481576 terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      " terminate due to over-size\n",
      "604658 terminate due to over-size\n",
      " terminate due to over-size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oasis/projects/nsf/csd181/yuncong/virtualenv-1.9.1/yuncongve/lib/python2.7/site-packages/skimage/util/dtype.py:111: UserWarning: Possible precision loss when converting from float64 to uint8\n",
      "  \"%s to %s\" % (dtypeobj_in, dtypeobj))\n"
     ]
    }
   ],
   "source": [
    "# for section in range(139, 144):\n",
    "for section in [65]:\n",
    "    \n",
    "    search_area_ellipses = {}\n",
    "    detected_clusters = {}\n",
    "\n",
    "#     for label in ['Pn', '7n', '7N']:\n",
    "    for label in ['PrVd']:\n",
    "\n",
    "        ks = np.array(sorted(label_position[label].keys()))\n",
    "\n",
    "        ds = ks - section\n",
    "            \n",
    "        if all(ds>=0):\n",
    "            next_labeled_section = ks[ds >= 0][0]\n",
    "            v1,v2,s1,s2,c0 = label_position[label][next_labeled_section]\n",
    "        elif all(ds<=0):\n",
    "            prev_labeled_section = ks[ds <= 0][-1]\n",
    "            v1,v2,s1,s2,c0 = label_position[label][prev_labeled_section]\n",
    "        else:\n",
    "            next_labeled_section = ks[ds >= 0][0]\n",
    "            prev_labeled_section = ks[ds <= 0][-1]\n",
    "\n",
    "            print 'prev_labeled_section', prev_labeled_section, 'next_labeled_section', next_labeled_section\n",
    "\n",
    "            if abs(section - prev_labeled_section) < abs(section - next_labeled_section):\n",
    "                v1,v2,s1,s2,c0 = label_position[label][prev_labeled_section]\n",
    "            else:\n",
    "                v1,v2,s1,s2,c0 = label_position[label][next_labeled_section]\n",
    "        \n",
    "        search_area_ellipses[label] = (v1,v2,s1,s2,c0)\n",
    "\n",
    "        angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "        ell_vertices = cv2.ellipse2Poly(tuple(c0.astype(np.int)), (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, 10)\n",
    "\n",
    "        sps = np.where([Path(ell_vertices).contains_point(s) for s in dms[section].sp_centroids[:,::-1]])[0]\n",
    "\n",
    "        print '%d sps to look at\\n' % len(sps)\n",
    "\n",
    "        all_unique_tuples = grow_clusters_from_sps(section, sps)\n",
    "\n",
    "        if all(ds<=0) or abs(section - prev_labeled_section) < abs(section - next_labeled_section):\n",
    "            clusters_sorted = sort_clusters(section, all_unique_tuples, label, prev_labeled_section)\n",
    "        else:\n",
    "            clusters_sorted = sort_clusters(section, all_unique_tuples, label, next_labeled_section)\n",
    "\n",
    "        dm = dms[section]\n",
    "\n",
    "        detected_clusters[label] = clusters_sorted[0]\n",
    "                \n",
    "        close_up = img_as_ubyte(dm.visualize_cluster(clusters_sorted[0], tight=True, bg='originalImage'))\n",
    "        imsave('/tmp/closeup_%04d_%s.jpg'%(section, label), close_up)\n",
    "\n",
    "\n",
    "    detected_cluster_edges = dict([(l, dm.find_boundary_dedges_ordered(cl)) for l, cl in detected_clusters.iteritems()])\n",
    "\n",
    "    atlas = img_as_ubyte(dm.visualize_edge_sets(detected_cluster_edges.values(), bg='originalImage'))\n",
    "    \n",
    "#     for label in ['Pn', '7n', '7N']:\n",
    "    for label in ['PrVd']:\n",
    "        v1,v2,s1,s2,c0 = search_area_ellipses[label]\n",
    "\n",
    "#         angle = np.rad2deg(np.arctan2(v1[1], v1[0]))\n",
    "#         cv2.ellipse(atlas, tuple(c0.astype(np.int)-(dm.xmin, dm.ymin)), \n",
    "#                     (int(2*1.5*s1), int(2*1.5*s2)), int(angle), 0, 360, (0,255,0), 10)\n",
    "\n",
    "        cy, cx = dm.sp_centroids[detected_clusters[label]].mean(axis=0)\n",
    "\n",
    "        cv2.putText(atlas, label, (int(cx)-dm.xmin, int(cy)-dm.ymin), cv2.FONT_HERSHEY_DUPLEX,\n",
    "                                3, ((255,0,0)), 3)\n",
    "\n",
    "    imsave('/tmp/labeled_%04d_allLandmarks.jpg'%section, atlas)\n",
    "\n",
    "    pickle.dump(search_area_ellipses, open('/tmp/search_area_ellipses_%04d.pkl'%section, 'w'))\n",
    "    pickle.dump(detected_clusters, open('/tmp/detected_clusters_%04d.pkl'%section, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
