{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuncong/MouseBrainAtlas/src/utilities/utilities2015.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-fc4079e49eb7>\", line 14, in <module>\n",
      "    get_ipython().magic(u'matplotlib inline')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n",
      "ENABLE_UPLOAD_S3 is not set, default to False.\n",
      "ENABLE_DOWNLOAD_S3 is not set, default to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Precision WorkStation\n",
      "{'MD589': 0.46, 'MD585': 0.46, 'MD594': 0.46, 'DEMO998': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5500: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s anchor: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 anchor: ini file /media/yuncong/BstemAtlasData/atlas_data/CSHL_data_processed/DEMO998/operation_configs/from_none_to_aligned.ini does not exist.\n",
      "File does not exist: /media/yuncong/BstemAtlasData/atlas_data/CSHL_data_processed/DEMO998/DEMO998_sorted_filenames.txt\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5506: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s sections_to_filenames: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 sections_to_filenames: \n",
      "File does not exist: /media/yuncong/BstemAtlasData/atlas_data/CSHL_data_processed/DEMO998/DEMO998_sorted_filenames.txt\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5517: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s filenames_to_sections: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 filenames_to_sections: \n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5522: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s section_limits: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 section_limits: ini file /media/yuncong/BstemAtlasData/atlas_data/CSHL_data_processed/DEMO998/operation_configs/from_none_to_aligned.ini does not exist.\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5528: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s cropbox: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 cropbox: ini file /media/yuncong/BstemAtlasData/atlas_data/CSHL_data_processed/DEMO998/operation_configs/from_none_to_aligned.ini does not exist.\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5538: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s valid_sections/filenames: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 valid_sections/filenames: DEMO998\n",
      "No anchor.txt is found. Seems we are using the operation ini to provide anchor. Try to load operation ini.\n",
      "/home/yuncong/MouseBrainAtlas/src/utilities/data_manager.py:5549: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  sys.stderr.write(\"Failed to cache %s image_shape: %s\\n\" % (stack, e.message))\n",
      "Failed to cache DEMO998 image_shape: ini file /media/yuncong/BstemAtlasData/atlas_data/CSHL_data_processed/DEMO998/operation_configs/from_none_to_aligned.ini does not exist.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    import mxnet as mx\n",
    "except:\n",
    "    sys.stderr.write(\"Cannot import mxnet.\\n\")\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import rotate\n",
    "\n",
    "sys.path.append(os.environ['REPO_DIR'] + '/utilities')\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from distributed_utilities import *\n",
    "from visualization_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MD585': (16384, 12000), 'MD589': (15520, 11936), 'MD594': (17216, 11104)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_cache['image_shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "\n",
    "sys.path.append('/home/yuncong/csd395/xgboost/python-package')\n",
    "try:\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "except:\n",
    "    sys.stderr.write('xgboost is not loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n",
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:53: UserWarning: \u001b[91mYou created Module with Module(..., label_names=['softmax_label']) but input with name 'softmax_label' is not found in symbol.list_arguments(). Did you mean one of:\n",
      "\tdata\u001b[0m\n",
      "  warnings.warn(msg)\n",
      "/usr/local/lib/python2.7/dist-packages/mxnet/module/base_module.py:65: UserWarning: Data provided by label_shapes don't match names specified by label_names ([] vs. ['softmax_label'])\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "model_dir_name = 'inception-bn-blue'\n",
    "model_name = 'inception-bn-blue'\n",
    "model, mean_img = load_mxnet_model(model_dir_name=model_dir_name, model_name=model_name, \n",
    "                                   num_gpus=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stacks = ['MD589', 'MD585']\n",
    "test_stacks = ['MD594']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_stain = {'MD585': 'N', 'MD589': 'N', 'MD594': 'N'}\n",
    "\n",
    "# Number of sections on which to sample examples from.\n",
    "stack_section_number = defaultdict(dict)\n",
    "\n",
    "for name_u in all_known_structures:\n",
    "    for st in train_stacks:\n",
    "        stack_section_number[st][name_u] = 10\n",
    "#         if name_u == '4N' or name_u == '10N':\n",
    "#             stack_section_number[st][name_u] = 20\n",
    "#         else:\n",
    "#             stack_section_number[st][name_u] = 10\n",
    "    for st in test_stacks:\n",
    "        stack_section_number[st][name_u] = 10\n",
    "\n",
    "stack_section_number.default_factory = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'patch_size': 224, 'spacing': 56},\n",
       " 2: {'comment': 'larger margin', 'patch_size': 224, 'spacing': 56},\n",
       " 3: {'comment': 'smaller spacing', 'patch_size': 224, 'spacing': 32},\n",
       " 4: {'comment': 'smaller spacing', 'patch_size': 224, 'spacing': 128},\n",
       " 5: {'comment': 'smaller spacing', 'patch_size': 224, 'spacing': 64},\n",
       " 6: {'comment': 'twice as large patch', 'patch_size': 448, 'spacing': 64},\n",
       " 7: {'comment': 'specify size/spacing in terms of microns rather than pixels',\n",
       "  'patch_size_um': 103.04,\n",
       "  'spacing_um': 30},\n",
       " 8: {'comment': 'larger patch', 'patch_size_um': 206.08, 'spacing_um': 30},\n",
       " 9: {'comment': 'larger patch', 'patch_size_um': 412.16, 'spacing_um': 30},\n",
       " 10: {'comment': 'larger patch', 'patch_size_um': 824.32, 'spacing_um': 30},\n",
       " 11: {'comment': 'larger patch', 'patch_size_um': 51.52, 'spacing_um': 30},\n",
       " 12: {'comment': 'larger patch', 'patch_size_um': 25.76, 'spacing_um': 30}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windowing_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "win_id = 7\n",
    "# win_id = 8\n",
    "# win_id = 9\n",
    "# win_id = 11\n",
    "# win_id = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n",
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD589_annotation_contours_05302018171048.hdf\n",
      "MD589_annotation_win11_05302018171048_grid_indices_lookup.hdf\n",
      "MD589_annotation_win11_05302018171048_grid_indices_lookup.hdf None\n",
      "MD589_annotation_win7_05302018171048_grid_indices_lookup.hdf\n",
      "MD589_annotation_win8_05302018171048_grid_indices_lookup.hdf\n",
      "MD589_annotation_win8_05302018171048_grid_indices_lookup.hdf None\n",
      "latest timestamp:  05302018171048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n",
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD585_annotation_contours_05302018161849.hdf\n",
      "MD585_annotation_win11_05302018161849_grid_indices_lookup.hdf\n",
      "MD585_annotation_win11_05302018161849_grid_indices_lookup.hdf None\n",
      "MD585_annotation_win7_05302018161849_grid_indices_lookup.hdf\n",
      "MD585_annotation_win8_05302018161849_grid_indices_lookup.hdf\n",
      "MD585_annotation_win8_05302018161849_grid_indices_lookup.hdf None\n",
      "latest timestamp:  05302018161849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n",
      "ENABLE_DOWNLOAD_S3 is False. Skip downloading from S3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD594_annotation_contours_05302018203315.hdf\n",
      "MD594_annotation_win11_05302018203315_grid_indices_lookup.hdf\n",
      "MD594_annotation_win11_05302018203315_grid_indices_lookup.hdf None\n",
      "MD594_annotation_win7_05302018203315_grid_indices_lookup.hdf\n",
      "MD594_annotation_win8_05302018203315_grid_indices_lookup.hdf\n",
      "MD594_annotation_win8_05302018203315_grid_indices_lookup.hdf None\n",
      "latest timestamp:  05302018203315\n"
     ]
    }
   ],
   "source": [
    "grid_indices_lookup_allStacks = {}\n",
    "\n",
    "for stack in train_stacks + test_stacks:\n",
    "#     try:\n",
    "#         grid_indices_lookup_allStacks[stack] = \\\n",
    "#         DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "#                                                            by_human=False, timestamp='latest',\n",
    "#                                                            detector_id_f=1,\n",
    "#                                                           return_locations=True)            \n",
    "\n",
    "#         grid_indices_lookup_allStacks[stack] = \\\n",
    "#         DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "#                                                            by_human=True, timestamp='latest',\n",
    "#                                                           return_locations=True, suffix='structures')            \n",
    "\n",
    "    grid_indices_lookup_allStacks[stack] = \\\n",
    "    DataManager.load_annotation_to_grid_indices_lookup(stack=stack, win_id=win_id,\n",
    "                                                       by_human=True, timestamp='latest',\n",
    "                                                      return_locations=True)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         sys.stderr.write(\"Fail to load annotation grid lookup for %s: %s\\n\" % (stack, e.message))\n",
    "#         raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "all_labels = sorted(list(set(chain.from_iterable(set(grid_indices_lookup_allStacks[st].columns.tolist()) \n",
    "                                                 for st in train_stacks + test_stacks))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_addresses(stacks, structure):\n",
    "        \n",
    "    positive_labels = [structure]\n",
    "    negative_labels = get_negative_labels(structure, 'neg_has_all_surround', \n",
    "                                          margin='500um', labels_found=all_labels)\n",
    "\n",
    "    positive_addresses_all_stacks = {}\n",
    "    negative_addresses_all_stacks = {}\n",
    "        \n",
    "    for stack in stacks:\n",
    "\n",
    "        candidate_sections = list(chain(*[grid_indices_lookup_allStacks[stack][pl].dropna(how='any').index.tolist() \n",
    "                                      for pl in positive_labels]))\n",
    "\n",
    "        n_sections = stack_section_number[stack][structure]\n",
    "\n",
    "        if stack_stain[stack] == 'F':\n",
    "            neurotrace_sections = []\n",
    "            nissl_sections = []\n",
    "            for sec in candidate_sections:\n",
    "                if (metadata_cache['sections_to_filenames'][stack][sec].split('-')[1][0] == 'F') \\\n",
    "                or stack in ['CHATM2', 'CHATM3']:\n",
    "                    neurotrace_sections.append(sec)\n",
    "                else:\n",
    "                    nissl_sections.append(sec)\n",
    "            print 'neurotrace_sections', neurotrace_sections\n",
    "            print 'nissl_sections', nissl_sections\n",
    "            sampled_sections = np.random.choice(neurotrace_sections, min(len(neurotrace_sections), n_sections), replace=False)\n",
    "        else:\n",
    "            sampled_sections = np.random.choice(candidate_sections, min(len(candidate_sections), n_sections), replace=False)\n",
    "            \n",
    "        positive_addresses_all_stacks[stack] = sorted([(stack, sec, tuple(loc))\n",
    "for nl in set(positive_labels) & set(grid_indices_lookup_allStacks[stack].columns)\n",
    "  for sec, locs in grid_indices_lookup_allStacks[stack][nl].loc[sampled_sections].dropna().iteritems()\n",
    "  for loc in locs])\n",
    "\n",
    "        negative_addresses_all_stacks[stack] = sorted([(stack, sec, tuple(loc))\n",
    "for nl in set(negative_labels) & set(grid_indices_lookup_allStacks[stack].columns)\n",
    "  for sec, locs in grid_indices_lookup_allStacks[stack][nl].loc[sampled_sections].dropna().iteritems()\n",
    "  for loc in locs])\n",
    "\n",
    "    positive_addresses = sum(positive_addresses_all_stacks.values(), [])\n",
    "    negative_addresses = sum(negative_addresses_all_stacks.values(), [])\n",
    "\n",
    "    del positive_addresses_all_stacks, negative_addresses_all_stacks\n",
    "\n",
    "    return positive_addresses, negative_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_scheme = 'stretch_min_max'\n",
    "# train_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "# train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "# train_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "train_scheme = 'none'\n",
    "\n",
    "# test_scheme = 'stretch_min_max'\n",
    "# test_scheme = 'normalize_mu_sigma_global_(-1,5)'\n",
    "# test_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,5)'\n",
    "# test_scheme = 'normalize_mu_region_sigma_wholeImage_(-1,9)'\n",
    "test_scheme = 'none'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# existing_classifier_id = 998 # If set, not train from scratch\n",
    "# existing_classifier_id = 990\n",
    "# extract_train_features = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "existing_classifier_id = None\n",
    "extract_train_features = True\n",
    "# classifier_id = 990 # MD585/MD589 -> MD594, cnn\n",
    "# classifier_id = 991 # MD585/MD589 -> MD594, lbp\n",
    "# classifier_id = 992 # MD585/MD589 -> MD594, glcm\n",
    "# classifier_id = 993 # MD585/MD589 -> MD594, mean greylevel\n",
    "# classifier_id = 899 # CHATM2 -> CHATM2, cnn\n",
    "# classifier_id = 898 # DEMO998 -> DEMO998, cnn, win7\n",
    "# classifier_id = 897 # DEMO998 -> DEMO998, cnn, win8\n",
    "# classifier_id = 896 # DEMO998 -> DEMO998, cnn, win9\n",
    "# win10 is too large and no patch can be extracted from DEMO998 section 225 3N.\n",
    "# classifier_id = 895 # DEMO998 -> DEMO998, cnn, win11\n",
    "# classifier_id = 894 # MD585/MD589 -> MD594, cnn, win8\n",
    "# classifier_id = 893 # MD585/MD589 -> MD594, cnn, win11\n",
    "classifier_id = 892 # MD585/MD589 -> MD594, cnn, concat 100um 200um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# method = 'mean_intensity'\n",
    "method = 'cnn' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prep_id = 'alignedBrainstemCrop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD594 92\n",
      "MD594 93\n",
      "MD594 94\n",
      "MD594 95\n",
      "MD594 96\n",
      "MD594 97\n",
      "MD594 98\n",
      "MD594 99\n",
      "MD594 100\n",
      "MD594 101\n",
      "MD594 102\n",
      "MD594 103\n",
      "MD594 104\n",
      "MD594 105\n",
      "MD594 106\n",
      "MD594 107\n",
      "MD594 108\n",
      "MD594 109\n",
      "MD594 110\n",
      "MD594 111\n",
      "MD594 112\n",
      "MD594 113\n",
      "MD594 114\n",
      "MD594 115\n",
      "MD594 116\n",
      "MD594 117\n",
      "MD594 118\n",
      "MD594 119\n",
      "MD594 120\n",
      "MD594 121\n",
      "MD594 122\n",
      "MD594 123\n",
      "MD594 124\n",
      "MD594 125\n",
      "MD594 126\n",
      "MD594 127\n",
      "MD594 128\n",
      "MD594 129\n",
      "MD594 130\n",
      "MD594 132\n",
      "MD594 133\n",
      "MD594 134\n",
      "MD594 135\n",
      "MD594 136\n",
      "MD594 137\n",
      "MD594 138\n",
      "MD594 139\n",
      "MD594 140\n",
      "MD594 141\n",
      "MD594 142\n",
      "MD594 143\n",
      "MD594 144\n",
      "MD594 145\n",
      "MD594 146\n",
      "MD594 147\n",
      "MD594 148\n",
      "MD594 149\n",
      "MD594 150\n",
      "MD594 151\n",
      "MD594 152\n",
      "MD594 153\n",
      "MD594 154\n",
      "MD594 155\n",
      "MD594 156\n",
      "MD594 157\n",
      "MD594 158\n",
      "MD594 159\n",
      "MD594 160\n",
      "MD594 161\n",
      "MD594 162\n",
      "MD594 163\n",
      "MD594 164\n",
      "MD594 165\n",
      "MD594 166\n",
      "MD594 167\n",
      "MD594 168\n",
      "MD594 169\n",
      "MD594 170\n",
      "MD594 171\n",
      "MD594 172\n",
      "MD594 173\n",
      "MD594 174\n",
      "MD594 175\n",
      "MD594 176\n",
      "MD594 177\n",
      "MD594 178\n",
      "MD594 179\n",
      "MD594 180\n",
      "MD594 181\n",
      "MD594 182\n",
      "MD594 183\n",
      "MD594 184\n",
      "MD594 185\n",
      "MD594 186\n",
      "MD594 188\n",
      "MD594 189\n",
      "MD594 190\n",
      "MD594 191\n",
      "MD594 192\n",
      "MD594 194\n",
      "MD594 195\n",
      "MD594 196\n",
      "MD594 197\n",
      "MD594 198\n",
      "MD594 199\n",
      "MD594 200\n",
      "MD594 201\n",
      "MD594 202\n",
      "MD594 203\n",
      "MD594 204\n",
      "MD594 205\n",
      "MD594 206\n",
      "MD594 207\n",
      "MD594 208\n",
      "MD594 209\n",
      "MD594 210\n",
      "MD594 211\n",
      "MD594 212\n",
      "MD594 213\n",
      "MD594 214\n",
      "MD594 215\n",
      "MD594 216\n",
      "MD594 217\n",
      "MD594 218\n",
      "MD594 219\n",
      "MD594 220\n",
      "MD594 221\n",
      "MD594 222\n",
      "MD594 223\n",
      "MD594 224\n",
      "MD594 225\n",
      "MD594 226\n",
      "MD594 227\n",
      "MD594 228\n",
      "MD594 229\n",
      "MD594 230\n",
      "MD594 231\n",
      "MD594 232\n",
      "MD594 233\n",
      "MD594 234\n",
      "MD594 235\n",
      "MD594 236\n",
      "MD594 237\n",
      "MD594 239\n",
      "MD594 240\n",
      "MD594 241\n",
      "MD594 242\n",
      "MD594 243\n",
      "MD594 244\n",
      "MD594 245\n",
      "MD594 246\n",
      "MD594 247\n",
      "MD594 248\n",
      "MD594 249\n",
      "MD594 250\n",
      "MD594 251\n",
      "MD594 252\n",
      "MD594 253\n",
      "MD594 254\n",
      "MD594 255\n",
      "MD594 256\n",
      "MD594 257\n",
      "MD594 258\n",
      "MD594 259\n",
      "MD594 260\n",
      "MD594 262\n",
      "MD594 263\n",
      "MD594 264\n",
      "MD594 265\n",
      "MD594 266\n",
      "MD594 267\n",
      "MD594 268\n",
      "MD594 269\n",
      "MD594 270\n",
      "MD594 271\n",
      "MD594 272\n",
      "MD594 273\n",
      "MD594 274\n",
      "MD594 275\n",
      "MD594 276\n",
      "MD594 277\n",
      "MD594 278\n",
      "MD594 279\n",
      "MD594 280\n",
      "MD594 281\n",
      "MD594 282\n",
      "MD594 283\n",
      "MD594 284\n",
      "MD594 285\n",
      "MD594 286\n",
      "MD594 287\n",
      "MD594 288\n",
      "MD594 289\n",
      "MD594 290\n",
      "MD594 291\n",
      "MD594 292\n",
      "MD594 293\n",
      "MD594 294\n",
      "MD594 295\n",
      "MD594 296\n",
      "MD594 297\n",
      "MD594 298\n",
      "MD594 299\n",
      "MD594 300\n",
      "MD594 301\n",
      "MD594 302\n",
      "MD594 303\n",
      "MD594 304\n",
      "MD594 305\n",
      "MD594 306\n",
      "MD594 307\n",
      "MD594 308\n",
      "MD594 309\n",
      "MD594 310\n",
      "MD594 311\n",
      "MD594 312\n",
      "MD594 313\n",
      "MD594 314\n",
      "MD594 315\n",
      "MD594 316\n",
      "MD594 317\n",
      "MD594 318\n",
      "MD594 319\n",
      "MD594 320\n",
      "MD594 321\n",
      "MD594 322\n",
      "MD594 323\n",
      "MD594 324\n",
      "MD594 325\n",
      "MD594 326\n",
      "MD594 327\n",
      "MD594 328\n",
      "MD594 329\n",
      "MD594 330\n",
      "MD594 331\n",
      "MD594 332\n",
      "MD594 333\n",
      "MD594 334\n",
      "MD594 335\n",
      "MD594 336\n",
      "MD594 337\n",
      "MD594 338\n",
      "MD594 339\n",
      "MD594 340\n",
      "MD594 342\n",
      "MD594 343\n",
      "MD594 344\n",
      "MD594 345\n",
      "MD594 346\n",
      "MD594 347\n",
      "MD594 348\n",
      "MD594 349\n",
      "MD594 350\n",
      "MD594 351\n",
      "MD594 352\n",
      "MD594 353\n",
      "MD594 354\n",
      "MD594 355\n",
      "MD594 356\n",
      "MD594 357\n",
      "MD594 358\n",
      "MD594 359\n",
      "MD594 360\n",
      "MD594 361\n",
      "MD594 362\n",
      "MD594 363\n",
      "MD594 364\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD594']:\n",
    "    \n",
    "    target_locations = grid_parameters_to_sample_locations(grid_spec=win_id_to_gridspec(win_id=7, stack=stack))\n",
    "    source_locations = grid_parameters_to_sample_locations(grid_spec=win_id_to_gridspec(win_id=8, stack=stack))\n",
    "    target_locations_mapped_to_source_locations, target_indices_to_source_indices_mapping = align_grid_specs(source_locations=source_locations, target_locations=target_locations, stack=stack)\n",
    "\n",
    "    target_locations_to_source_locations_mapping = {tuple(tgt_loc): tuple(src_loc)\n",
    "                        for tgt_loc, src_loc in zip(target_locations, target_locations_mapped_to_source_locations)}\n",
    "    \n",
    "    for sec in metadata_cache['valid_sections'][stack]:\n",
    "        \n",
    "        print stack, sec\n",
    "        \n",
    "        features_100um, locations_100um = DataManager.load_dnn_features_v2(stack=stack, sec=sec,\n",
    "                                                                           prep_id=prep_id, win_id=7,\n",
    "                                                                           normalization_scheme='none',\n",
    "                                                                           model_name=model_name)\n",
    "        \n",
    "        features_200um, locations_200um = DataManager.load_dnn_features_v2(stack=stack, sec=sec,\n",
    "                                                               prep_id=prep_id, win_id=8,\n",
    "                                                               normalization_scheme='none',\n",
    "                                                               model_name=model_name)\n",
    "\n",
    "        locations_200um_to_indices_200um_in_loaded_pool_mapping = {tuple(loc_200um): idx_200um_in_loaded_pool\n",
    "                                for idx_200um_in_loaded_pool, loc_200um in enumerate(locations_200um)}\n",
    "        \n",
    "        features = []\n",
    "        locations = []\n",
    "        for idx_100um_in_loaded_pool, loc_100um in enumerate(locations_100um):\n",
    "            loc_200um = target_locations_to_source_locations_mapping[tuple(loc_100um)]\n",
    "            if tuple(loc_200um) in locations_200um_to_indices_200um_in_loaded_pool_mapping:\n",
    "                # check if the 100um grid mapped to a 200um grid that has features associated\n",
    "                idx_200um_in_loaded_pool = locations_200um_to_indices_200um_in_loaded_pool_mapping[tuple(loc_200um)]\n",
    "\n",
    "                f = np.concatenate([features_100um[idx_100um_in_loaded_pool], \n",
    "                                   features_200um[idx_200um_in_loaded_pool]], axis=-1)\n",
    "                features.append(f)\n",
    "                locations.append(locations_100um[idx_100um_in_loaded_pool])\n",
    "                \n",
    "        DataManager.save_dnn_features_v2(features=features, locations=locations,\n",
    "                         stack=stack, sec=sec, prep_id=prep_id,\n",
    "                         win_id=7, normalization_scheme='none',\n",
    "                         model_name='concat_100um_200um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD585 83\n",
      "MD585 84\n",
      "MD585 85\n",
      "MD585 86\n",
      "MD585 87\n",
      "MD585 88\n",
      "MD585 89\n",
      "MD585 90\n",
      "MD585 91\n",
      "MD585 92\n",
      "MD585 93\n",
      "MD585 94\n",
      "MD585 95\n",
      "MD585 96\n",
      "MD585 97\n",
      "MD585 98\n",
      "MD585 99\n",
      "MD585 100\n",
      "MD585 101\n",
      "MD585 102\n",
      "MD585 103\n",
      "MD585 104\n",
      "MD585 105\n",
      "MD585 106\n",
      "MD585 107\n",
      "MD585 108\n",
      "MD585 109\n",
      "MD585 110\n",
      "MD585 111\n",
      "MD585 112\n",
      "MD585 113\n",
      "MD585 114\n",
      "MD585 115\n",
      "MD585 116\n",
      "MD585 117\n",
      "MD585 119\n",
      "MD585 120\n",
      "MD585 121\n",
      "MD585 122\n",
      "MD585 123\n",
      "MD585 124\n",
      "MD585 125\n",
      "MD585 126\n",
      "MD585 127\n",
      "MD585 128\n",
      "MD585 129\n",
      "MD585 130\n",
      "MD585 131\n",
      "MD585 132\n",
      "MD585 133\n",
      "MD585 134\n",
      "MD585 135\n",
      "MD585 136\n",
      "MD585 137\n",
      "MD585 138\n",
      "MD585 139\n",
      "MD585 140\n",
      "MD585 141\n",
      "MD585 142\n",
      "MD585 143\n",
      "MD585 144\n",
      "MD585 145\n",
      "MD585 146\n",
      "MD585 147\n",
      "MD585 148\n",
      "MD585 149\n",
      "MD585 150\n",
      "MD585 151\n",
      "MD585 152\n",
      "MD585 153\n",
      "MD585 154\n",
      "MD585 155\n",
      "MD585 156\n",
      "MD585 157\n",
      "MD585 158\n",
      "MD585 159\n",
      "MD585 160\n",
      "MD585 161\n",
      "MD585 162\n",
      "MD585 163\n",
      "MD585 164\n",
      "MD585 165\n",
      "MD585 166\n",
      "MD585 167\n",
      "MD585 168\n",
      "MD585 169\n",
      "MD585 170\n",
      "MD585 171\n",
      "MD585 172\n",
      "MD585 173\n",
      "MD585 174\n",
      "MD585 175\n",
      "MD585 176\n",
      "MD585 177\n",
      "MD585 178\n",
      "MD585 179\n",
      "MD585 180\n",
      "MD585 181\n",
      "MD585 182\n",
      "MD585 183\n",
      "MD585 184\n",
      "MD585 185\n",
      "MD585 186\n",
      "MD585 187\n",
      "MD585 188\n",
      "MD585 189\n",
      "MD585 190\n",
      "MD585 191\n",
      "MD585 192\n",
      "MD585 193\n",
      "MD585 194\n",
      "MD585 195\n",
      "MD585 196\n",
      "MD585 197\n",
      "MD585 198\n",
      "MD585 199\n",
      "MD585 200\n",
      "MD585 201\n",
      "MD585 202\n",
      "MD585 203\n",
      "MD585 204\n",
      "MD585 205\n",
      "MD585 206\n",
      "MD585 207\n",
      "MD585 208\n",
      "MD585 209\n",
      "MD585 210\n",
      "MD585 211\n",
      "MD585 212\n",
      "MD585 213\n",
      "MD585 214\n",
      "MD585 215\n",
      "MD585 216\n",
      "MD585 217\n",
      "MD585 218\n",
      "MD585 219\n",
      "MD585 220\n",
      "MD585 221\n",
      "MD585 222\n",
      "MD585 223\n",
      "MD585 224\n",
      "MD585 225\n",
      "MD585 226\n",
      "MD585 227\n",
      "MD585 228\n",
      "MD585 229\n",
      "MD585 230\n",
      "MD585 231\n",
      "MD585 232\n",
      "MD585 233\n",
      "MD585 234\n",
      "MD585 235\n",
      "MD585 236\n",
      "MD585 237\n",
      "MD585 238\n",
      "MD585 239\n",
      "MD585 240\n",
      "MD585 241\n",
      "MD585 242\n",
      "MD585 243\n",
      "MD585 244\n",
      "MD585 245\n",
      "MD585 246\n",
      "MD585 247\n",
      "MD585 248\n",
      "MD585 249\n",
      "MD585 250\n",
      "MD585 251\n",
      "MD585 252\n",
      "MD585 253\n",
      "MD585 254\n",
      "MD585 255\n",
      "MD585 256\n",
      "MD585 257\n",
      "MD585 258\n",
      "MD585 259\n",
      "MD585 260\n",
      "MD585 261\n",
      "MD585 262\n",
      "MD585 263\n",
      "MD585 264\n",
      "MD585 265\n",
      "MD585 266\n",
      "MD585 267\n",
      "MD585 268\n",
      "MD585 269\n",
      "MD585 270\n",
      "MD585 271\n",
      "MD585 272\n",
      "MD585 273\n",
      "MD585 274\n",
      "MD585 275\n",
      "MD585 276\n",
      "MD585 277\n",
      "MD585 278\n",
      "MD585 279\n",
      "MD585 280\n",
      "MD585 281\n",
      "MD585 282\n",
      "MD585 283\n",
      "MD585 284\n",
      "MD585 285\n",
      "MD585 286\n",
      "MD585 287\n",
      "MD585 288\n",
      "MD585 289\n",
      "MD585 290\n",
      "MD585 291\n",
      "MD585 292\n",
      "MD585 293\n",
      "MD585 294\n",
      "MD585 295\n",
      "MD585 296\n",
      "MD585 297\n",
      "MD585 298\n",
      "MD585 300\n",
      "MD585 301\n",
      "MD585 302\n",
      "MD585 303\n",
      "MD585 304\n",
      "MD585 305\n",
      "MD585 306\n",
      "MD585 307\n",
      "MD585 308\n",
      "MD585 309\n",
      "MD585 310\n",
      "MD585 311\n",
      "MD585 312\n",
      "MD585 313\n",
      "MD585 314\n",
      "MD585 315\n",
      "MD585 316\n",
      "MD585 317\n",
      "MD585 318\n",
      "MD585 319\n",
      "MD585 320\n",
      "MD585 321\n",
      "MD585 322\n",
      "MD585 323\n",
      "MD585 324\n",
      "MD585 325\n",
      "MD585 326\n",
      "MD585 327\n",
      "MD585 328\n",
      "MD585 329\n",
      "MD585 330\n",
      "MD585 331\n",
      "MD585 332\n",
      "MD585 333\n",
      "MD585 334\n",
      "MD585 335\n",
      "MD585 336\n",
      "MD585 337\n",
      "MD585 338\n",
      "MD585 339\n",
      "MD585 340\n",
      "MD585 341\n",
      "MD585 342\n",
      "MD585 343\n",
      "MD585 344\n",
      "MD585 345\n",
      "MD585 346\n",
      "MD585 347\n",
      "MD585 348\n",
      "MD585 349\n",
      "MD585 350\n",
      "MD585 351\n",
      "MD585 352\n",
      "MD589 92\n",
      "MD589 93\n",
      "MD589 94\n",
      "MD589 95\n",
      "MD589 96\n",
      "MD589 97\n",
      "MD589 98\n",
      "MD589 99\n",
      "MD589 100\n",
      "MD589 101\n",
      "MD589 102\n",
      "MD589 103\n",
      "MD589 104\n",
      "MD589 105\n",
      "MD589 106\n",
      "MD589 107\n",
      "MD589 108\n",
      "MD589 109\n",
      "MD589 110\n",
      "MD589 111\n",
      "MD589 112\n",
      "MD589 113\n",
      "MD589 114\n",
      "MD589 115\n",
      "MD589 116\n",
      "MD589 117\n",
      "MD589 119\n",
      "MD589 120\n",
      "MD589 121\n",
      "MD589 122\n",
      "MD589 123\n",
      "MD589 124\n",
      "MD589 125\n",
      "MD589 126\n",
      "MD589 127\n",
      "MD589 128\n",
      "MD589 129\n",
      "MD589 130\n",
      "MD589 131\n",
      "MD589 132\n",
      "MD589 133\n",
      "MD589 134\n",
      "MD589 135\n",
      "MD589 136\n",
      "MD589 137\n",
      "MD589 138\n",
      "MD589 139\n",
      "MD589 140\n",
      "MD589 141\n",
      "MD589 142\n",
      "MD589 143\n",
      "MD589 144\n",
      "MD589 145\n",
      "MD589 146\n",
      "MD589 147\n",
      "MD589 148\n",
      "MD589 149\n",
      "MD589 150\n",
      "MD589 151\n",
      "MD589 152\n",
      "MD589 153\n",
      "MD589 154\n",
      "MD589 155\n",
      "MD589 156\n",
      "MD589 157\n",
      "MD589 158\n",
      "MD589 159\n",
      "MD589 160\n",
      "MD589 161\n",
      "MD589 162\n",
      "MD589 163\n",
      "MD589 164\n",
      "MD589 165\n",
      "MD589 166\n",
      "MD589 167\n",
      "MD589 168\n",
      "MD589 169\n",
      "MD589 170\n",
      "MD589 171\n",
      "MD589 172\n",
      "MD589 173\n",
      "MD589 174\n",
      "MD589 175\n",
      "MD589 176\n",
      "MD589 177\n",
      "MD589 178\n",
      "MD589 179\n",
      "MD589 180\n",
      "MD589 181\n",
      "MD589 182\n",
      "MD589 183\n",
      "MD589 184\n",
      "MD589 185\n",
      "MD589 186\n",
      "MD589 187\n",
      "MD589 188\n",
      "MD589 189\n",
      "MD589 190\n",
      "MD589 191\n",
      "MD589 192\n",
      "MD589 193\n",
      "MD589 194\n",
      "MD589 195\n",
      "MD589 196\n",
      "MD589 197\n",
      "MD589 198\n",
      "MD589 199\n",
      "MD589 200\n",
      "MD589 201\n",
      "MD589 202\n",
      "MD589 203\n",
      "MD589 204\n",
      "MD589 205\n",
      "MD589 206\n",
      "MD589 207\n",
      "MD589 208\n",
      "MD589 209\n",
      "MD589 210\n",
      "MD589 211\n",
      "MD589 212\n",
      "MD589 213\n",
      "MD589 214\n",
      "MD589 215\n",
      "MD589 216\n",
      "MD589 217\n",
      "MD589 218\n",
      "MD589 219\n",
      "MD589 220\n",
      "MD589 222\n",
      "MD589 223\n",
      "MD589 224\n",
      "MD589 225\n",
      "MD589 226\n",
      "MD589 227\n",
      "MD589 228\n",
      "MD589 229\n",
      "MD589 230\n",
      "MD589 231\n",
      "MD589 232\n",
      "MD589 233\n",
      "MD589 234\n",
      "MD589 235\n",
      "MD589 236\n",
      "MD589 237\n",
      "MD589 238\n",
      "MD589 239\n",
      "MD589 240\n",
      "MD589 241\n",
      "MD589 242\n",
      "MD589 243\n",
      "MD589 244\n",
      "MD589 245\n",
      "MD589 246\n",
      "MD589 247\n",
      "MD589 248\n",
      "MD589 249\n",
      "MD589 250\n",
      "MD589 251\n",
      "MD589 252\n",
      "MD589 253\n",
      "MD589 254\n",
      "MD589 255\n",
      "MD589 256\n",
      "MD589 257\n",
      "MD589 258\n",
      "MD589 259\n",
      "MD589 260\n",
      "MD589 261\n",
      "MD589 262\n",
      "MD589 263\n",
      "MD589 264\n",
      "MD589 265\n",
      "MD589 266\n",
      "MD589 267\n",
      "MD589 268\n",
      "MD589 269\n",
      "MD589 270\n",
      "MD589 271\n",
      "MD589 272\n",
      "MD589 273\n",
      "MD589 274\n",
      "MD589 275\n",
      "MD589 276\n",
      "MD589 277\n",
      "MD589 278\n",
      "MD589 279\n",
      "MD589 280\n",
      "MD589 281\n",
      "MD589 282\n",
      "MD589 283\n",
      "MD589 284\n",
      "MD589 285\n",
      "MD589 286\n",
      "MD589 287\n",
      "MD589 288\n",
      "MD589 289\n",
      "MD589 290\n",
      "MD589 291\n",
      "MD589 292\n",
      "MD589 293\n",
      "MD589 294\n",
      "MD589 295\n",
      "MD589 296\n",
      "MD589 297\n",
      "MD589 298\n",
      "MD589 299\n",
      "MD589 301\n",
      "MD589 302\n",
      "MD589 303\n",
      "MD589 304\n",
      "MD589 305\n",
      "MD589 306\n",
      "MD589 307\n",
      "MD589 308\n",
      "MD589 309\n",
      "MD589 310\n",
      "MD589 311\n",
      "MD589 312\n",
      "MD589 313\n",
      "MD589 314\n",
      "MD589 315\n",
      "MD589 316\n",
      "MD589 317\n",
      "MD589 318\n",
      "MD589 319\n",
      "MD589 320\n",
      "MD589 321\n",
      "MD589 322\n",
      "MD589 323\n",
      "MD589 325\n",
      "MD589 326\n",
      "MD589 327\n",
      "MD589 328\n",
      "MD589 329\n",
      "MD589 330\n",
      "MD589 331\n",
      "MD589 332\n",
      "MD589 333\n",
      "MD589 334\n",
      "MD589 335\n",
      "MD589 336\n",
      "MD589 337\n",
      "MD589 338\n",
      "MD589 339\n",
      "MD589 340\n",
      "MD589 341\n",
      "MD589 342\n",
      "MD589 343\n",
      "MD589 344\n",
      "MD589 345\n",
      "MD589 346\n",
      "MD589 347\n",
      "MD589 348\n",
      "MD589 349\n",
      "MD589 350\n",
      "MD589 351\n",
      "MD589 352\n",
      "MD589 353\n",
      "MD589 354\n",
      "MD589 355\n",
      "MD589 356\n",
      "MD589 357\n",
      "MD589 358\n",
      "MD589 359\n",
      "MD589 360\n",
      "MD589 361\n",
      "MD589 362\n",
      "MD589 363\n",
      "MD589 364\n",
      "MD589 365\n",
      "MD589 366\n",
      "MD589 367\n",
      "MD589 368\n",
      "MD589 369\n",
      "MD589 370\n",
      "MD594 92\n",
      "MD594 93\n",
      "MD594 94\n",
      "MD594 95\n",
      "MD594 96\n",
      "MD594 97\n",
      "MD594 98\n",
      "MD594 99\n",
      "MD594 100\n",
      "MD594 101\n",
      "MD594 102\n",
      "MD594 103\n",
      "MD594 104\n",
      "MD594 105\n",
      "MD594 106\n",
      "MD594 107\n",
      "MD594 108\n",
      "MD594 109\n",
      "MD594 110\n",
      "MD594 111\n",
      "MD594 112\n",
      "MD594 113\n",
      "MD594 114\n",
      "MD594 115\n",
      "MD594 116\n",
      "MD594 117\n",
      "MD594 118\n",
      "MD594 119\n",
      "MD594 120\n",
      "MD594 121\n",
      "MD594 122\n",
      "MD594 123\n",
      "MD594 124\n",
      "MD594 125\n",
      "MD594 126\n",
      "MD594 127\n",
      "MD594 128\n",
      "MD594 129\n",
      "MD594 130\n",
      "MD594 132\n",
      "MD594 133\n",
      "MD594 134\n",
      "MD594 135\n",
      "MD594 136\n",
      "MD594 137\n",
      "MD594 138\n",
      "MD594 139\n",
      "MD594 140\n",
      "MD594 141\n",
      "MD594 142\n",
      "MD594 143\n",
      "MD594 144\n",
      "MD594 145\n",
      "MD594 146\n",
      "MD594 147\n",
      "MD594 148\n",
      "MD594 149\n",
      "MD594 150\n",
      "MD594 151\n",
      "MD594 152\n",
      "MD594 153\n",
      "MD594 154\n",
      "MD594 155\n",
      "MD594 156\n",
      "MD594 157\n",
      "MD594 158\n",
      "MD594 159\n",
      "MD594 160\n",
      "MD594 161\n",
      "MD594 162\n",
      "MD594 163\n",
      "MD594 164\n",
      "MD594 165\n",
      "MD594 166\n",
      "MD594 167\n",
      "MD594 168\n",
      "MD594 169\n",
      "MD594 170\n",
      "MD594 171\n",
      "MD594 172\n",
      "MD594 173\n",
      "MD594 174\n",
      "MD594 175\n",
      "MD594 176\n",
      "MD594 177\n",
      "MD594 178\n",
      "MD594 179\n",
      "MD594 180\n",
      "MD594 181\n",
      "MD594 182\n",
      "MD594 183\n",
      "MD594 184\n",
      "MD594 185\n",
      "MD594 186\n",
      "MD594 188\n",
      "MD594 189\n",
      "MD594 190\n",
      "MD594 191\n",
      "MD594 192\n",
      "MD594 194\n",
      "MD594 195\n",
      "MD594 196\n",
      "MD594 197\n",
      "MD594 198\n",
      "MD594 199\n",
      "MD594 200\n",
      "MD594 201\n",
      "MD594 202\n",
      "MD594 203\n",
      "MD594 204\n",
      "MD594 205\n",
      "MD594 206\n",
      "MD594 207\n",
      "MD594 208\n",
      "MD594 209\n",
      "MD594 210\n",
      "MD594 211\n",
      "MD594 212\n",
      "MD594 213\n",
      "MD594 214\n",
      "MD594 215\n",
      "MD594 216\n",
      "MD594 217\n",
      "MD594 218\n",
      "MD594 219\n",
      "MD594 220\n",
      "MD594 221\n",
      "MD594 222\n",
      "MD594 223\n",
      "MD594 224\n",
      "MD594 225\n",
      "MD594 226\n"
     ]
    }
   ],
   "source": [
    "for stack in ['MD585', 'MD589', 'MD594']:\n",
    "    \n",
    "    size100um_locations = grid_parameters_to_sample_locations(grid_spec=win_id_to_gridspec(win_id=7, stack=stack))\n",
    "    size200um_locations = grid_parameters_to_sample_locations(grid_spec=win_id_to_gridspec(win_id=8, stack=stack))\n",
    "    size50um_locations = grid_parameters_to_sample_locations(grid_spec=win_id_to_gridspec(win_id=11, stack=stack))\n",
    "    \n",
    "    size100um_locations_mapped_to_size200um_locations, \\\n",
    "    size100um_indices_to_size200um_indices_mapping = \\\n",
    "    align_grid_specs(source_locations=size200um_locations, \n",
    "                     target_locations=size100um_locations, \n",
    "                     stack=stack)\n",
    "\n",
    "    size100um_locations_to_size200um_locations_mapping = {tuple(tgt_loc): tuple(src_loc)\n",
    "                        for tgt_loc, src_loc in zip(size100um_locations, \n",
    "                                                    size100um_locations_mapped_to_size200um_locations)}\n",
    "    \n",
    "    size100um_locations_mapped_to_size50um_locations, \\\n",
    "    size100um_indices_to_size50um_indices_mapping = \\\n",
    "    align_grid_specs(source_locations=size50um_locations, \n",
    "                     target_locations=size100um_locations, \n",
    "                     stack=stack)\n",
    "    size100um_locations_to_size50um_locations_mapping = {tuple(tgt_loc): tuple(src_loc)\n",
    "                        for tgt_loc, src_loc in zip(size100um_locations, \n",
    "                                                    size100um_locations_mapped_to_size50um_locations)}\n",
    "    \n",
    "    for sec in metadata_cache['valid_sections'][stack]:\n",
    "        \n",
    "        print stack, sec\n",
    "        \n",
    "        features_100um, locations_100um = DataManager.load_dnn_features_v2(stack=stack, sec=sec,\n",
    "                                                                           prep_id=prep_id, win_id=7,\n",
    "                                                                           normalization_scheme='none',\n",
    "                                                                           model_name=model_name)\n",
    "        \n",
    "        features_200um, locations_200um = DataManager.load_dnn_features_v2(stack=stack, sec=sec,\n",
    "                                                               prep_id=prep_id, win_id=8,\n",
    "                                                               normalization_scheme='none',\n",
    "                                                               model_name=model_name)\n",
    "\n",
    "        features_50um, locations_50um = DataManager.load_dnn_features_v2(stack=stack, sec=sec,\n",
    "                                                               prep_id=prep_id, win_id=11,\n",
    "                                                               normalization_scheme='none',\n",
    "                                                               model_name=model_name)\n",
    "\n",
    "        locations_200um_to_indices_200um_in_loaded_pool_mapping = {tuple(loc_200um): idx_200um_in_loaded_pool\n",
    "                                for idx_200um_in_loaded_pool, loc_200um in enumerate(locations_200um)}\n",
    "\n",
    "        locations_50um_to_indices_50um_in_loaded_pool_mapping = {tuple(loc_50um): idx_50um_in_loaded_pool\n",
    "                        for idx_50um_in_loaded_pool, loc_50um in enumerate(locations_50um)}\n",
    "\n",
    "        \n",
    "        features = []\n",
    "        locations = []\n",
    "        for idx_100um_in_loaded_pool, loc_100um in enumerate(locations_100um):\n",
    "            loc_200um = size100um_locations_to_size200um_locations_mapping[tuple(loc_100um)]\n",
    "            loc_50um = size100um_locations_to_size50um_locations_mapping[tuple(loc_100um)]\n",
    "            \n",
    "            if tuple(loc_200um) in locations_200um_to_indices_200um_in_loaded_pool_mapping and \\\n",
    "            tuple(loc_50um) in locations_50um_to_indices_50um_in_loaded_pool_mapping:\n",
    "                \n",
    "                # check if the 100um grid mapped to a 200um grid that has features associated\n",
    "                idx_200um_in_loaded_pool = locations_200um_to_indices_200um_in_loaded_pool_mapping[tuple(loc_200um)]\n",
    "                \n",
    "                idx_50um_in_loaded_pool = locations_50um_to_indices_50um_in_loaded_pool_mapping[tuple(loc_50um)]\n",
    "\n",
    "                f = np.concatenate([features_100um[idx_100um_in_loaded_pool], \n",
    "                                   features_200um[idx_200um_in_loaded_pool],\n",
    "                                   features_50um[idx_50um_in_loaded_pool]], axis=-1)\n",
    "                features.append(f)\n",
    "                locations.append(locations_100um[idx_100um_in_loaded_pool])\n",
    "                \n",
    "        DataManager.save_dnn_features_v2(features=features, locations=locations,\n",
    "                         stack=stack, sec=sec, prep_id=prep_id,\n",
    "                         win_id=7, normalization_scheme='none',\n",
    "                         model_name='concat_100um_200um_50um')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_new_addresses = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'concat_100um_200um'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 1000\n",
      "Trial "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting classifier: 0.76 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Training: 1000 positive, 1000 negative\n",
      "Test: 755 positive, 1000 negative\n",
      "Trial 1\n",
      "Training: 1000 positive, 1000 negative\n",
      "Test: 755 positive, 1000 negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting classifier: 0.72 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2\n",
      "Training: 1000 positive, 1000 negative\n",
      "Test: 755 positive, 1000 negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting classifier: 0.68 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 5000\n",
      "Trial 0\n",
      "Training: 1335 positive, 1335 negative\n",
      "Test: 755 positive, 1000 negative\n"
     ]
    }
   ],
   "source": [
    "for structure in all_known_structures:\n",
    "# for structure in ['3N']:\n",
    "\n",
    "    # features_dict = {(scheme, tfv): {} for scheme in schemes for tfv in transforms}\n",
    "    features_dict = defaultdict(dict)\n",
    "\n",
    "    ############## Sample and Load training feature vectors #########################################\n",
    "\n",
    "    if extract_train_features:\n",
    "    \n",
    "        positive_addresses_traindata, negative_addresses_traindata = \\\n",
    "        sample_addresses(train_stacks, structure)\n",
    "\n",
    "        print '# positive train =', len(positive_addresses_traindata)\n",
    "        print '# negative train =', len(negative_addresses_traindata)\n",
    "\n",
    "        addresses_to_compute = positive_addresses_traindata + negative_addresses_traindata\n",
    "\n",
    "        for variant in [0]:\n",
    "            features_loaded = read_features(addresses=addresses_to_compute, \n",
    "                                            scheme=train_scheme, win_id=win_id, prep_id=prep_id, \n",
    "                                            model=model, mean_img=mean_img, model_name=model_name, \n",
    "                                            batch_size=batch_size,\n",
    "                                           method=method,\n",
    "                                           compute_new_addresses=compute_new_addresses\n",
    "                                           )\n",
    "            \n",
    "            for addr, f in izip(addresses_to_compute, features_loaded):\n",
    "                if f is not None:\n",
    "                    features_dict[(train_scheme, variant)][addr] = f\n",
    "\n",
    "            del features_loaded\n",
    "\n",
    "    ############## Sample and Load test feature vectors #############################################\n",
    "    \n",
    "    positive_addresses_testdata, negative_addresses_testdata = \\\n",
    "    sample_addresses(test_stacks, structure)\n",
    "    \n",
    "    print '# positive test =', len(positive_addresses_testdata)\n",
    "    print '# negative test =', len(negative_addresses_testdata)\n",
    "    \n",
    "    addresses_to_compute = positive_addresses_testdata + negative_addresses_testdata\n",
    "\n",
    "    for variant in [0]:\n",
    "        features_loaded = read_features(addresses=addresses_to_compute, \n",
    "                                        scheme=test_scheme, win_id=win_id, prep_id=prep_id, \n",
    "                                        model=model, mean_img=mean_img, model_name=model_name, \n",
    "                                        batch_size=batch_size,\n",
    "                                           method=method,\n",
    "                                       compute_new_addresses=compute_new_addresses\n",
    "                                       )\n",
    "\n",
    "        for addr, f in izip(addresses_to_compute, features_loaded):\n",
    "            if f is not None:\n",
    "                features_dict[(test_scheme, variant)][addr] = f\n",
    "\n",
    "        del features_loaded\n",
    "            \n",
    "    ########################################################################################\n",
    "    \n",
    "    # n_train_list = [10, 100, 200, 500, 1000, 2000, 5000, 10000, 15000]\n",
    "#     n_train_list = [10, 1000]\n",
    "    n_train_list = [1000, 5000, 15000]\n",
    "#     n_train_list = [1000, 5000]\n",
    "#     n_train_list = [15000]\n",
    "    test_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "    train_metrics_all_ntrain = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for n_train in n_train_list:\n",
    "\n",
    "        print \"n_train\", n_train\n",
    "        \n",
    "        for trial in range(3):\n",
    "            print \"Trial\", trial\n",
    "            \n",
    "            ##### Sample from training pool the required number of examples ######\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            n_train_pos = min(n_train, len(positive_addresses_traindata))\n",
    "#             if len(positive_addresses_traindata) < n_train_pos:\n",
    "#                 continue\n",
    "            training_pos_indices = np.random.choice(range(len(positive_addresses_traindata)), n_train_pos, replace=False)\n",
    "            \n",
    "            n_test_pos = min(len(positive_addresses_testdata), 1000)\n",
    "            test_pos_indices = np.random.choice(range(len(positive_addresses_testdata)), n_test_pos, replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_pos_total = len(positive_addresses)\n",
    "        #     n_train_pos = 1000\n",
    "        #     training_pos_indices = np.random.choice(range(n_pos_total), n_train_pos, replace=False)\n",
    "        #     test_pos_indices = np.random.choice(np.setdiff1d(range(n_pos_total), training_pos_indices),\n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_pos = len(test_pos_indices)\n",
    "\n",
    "            # If train and test data are from different sets\n",
    "            n_train_neg = n_train_pos\n",
    "            training_neg_indices = np.random.choice(range(len(negative_addresses_traindata)), n_train_neg, replace=False)\n",
    "            \n",
    "            n_test_neg = min(len(negative_addresses_testdata), 1000)\n",
    "            test_neg_indices = np.random.choice(range(len(negative_addresses_testdata)), n_test_neg, replace=False)\n",
    "\n",
    "            # If train and test are from same set\n",
    "        #     n_neg_total = len(negative_addresses)\n",
    "        #     n_train_neg = 1000\n",
    "        #     training_neg_indices = np.random.choice(range(n_neg_total), n_train_neg, replace=False)\n",
    "        #     test_neg_indices = np.random.choice(np.setdiff1d(range(n_neg_total), training_neg_indices), \n",
    "        #                                         size=min(2000, n_pos_total-n_train_pos), replace=False)\n",
    "        #     n_test_neg = len(test_neg_indices)\n",
    "\n",
    "            print \"Training: %d positive, %d negative\" % (n_train_pos, n_train_neg)\n",
    "            print \"Test: %d positive, %d negative\" % (n_test_pos, n_test_neg)\n",
    "\n",
    "            ################\n",
    "\n",
    "            if extract_train_features:\n",
    "                # If train and test data are from different sets\n",
    "                addresses_train_pos = [positive_addresses_traindata[i] for i in training_pos_indices]\n",
    "                addresses_train_neg = [negative_addresses_traindata[i] for i in training_neg_indices]\n",
    "\n",
    "            addresses_test_pos = [positive_addresses_testdata[i] for i in test_pos_indices]\n",
    "            addresses_test_neg = [negative_addresses_testdata[i] for i in test_neg_indices]\n",
    "\n",
    "            #################\n",
    "\n",
    "#             for augment_training in [True, False]:\n",
    "            for augment_training in [False]:\n",
    "        \n",
    "                feature_classifier_alg = 'lr'\n",
    "#                 feature_classifier_alg = 'xgb2'\n",
    "        #             feature_classifier_alg = 'lin_svc'\n",
    "        #             feature_classifier_alg = 'lin_svc_calib'\n",
    "                sample_weights = None   \n",
    "\n",
    "                if extract_train_features:\n",
    "            \n",
    "                    if augment_training:\n",
    "                        train_transforms = range(8)\n",
    "                    else:\n",
    "                        train_transforms = range(1)\n",
    "                    features_train_pos = {(train_scheme, tf_variant): \n",
    "                                          [features_dict[(train_scheme, tf_variant)][addr] \n",
    "                                           for addr in addresses_train_pos \n",
    "                                           if addr in features_dict[(train_scheme, tf_variant)]]\n",
    "                                              for tf_variant in train_transforms}\n",
    "                    features_train_neg = {(train_scheme, tf_variant): \n",
    "                                          [features_dict[(train_scheme, tf_variant)][addr] \n",
    "                                           for addr in addresses_train_neg\n",
    "                                           if addr in features_dict[(train_scheme, tf_variant)]]\n",
    "                                              for tf_variant in train_transforms}\n",
    "\n",
    "                    train_data = np.concatenate([np.r_[features_train_pos[(train_scheme,tf)], \n",
    "                                                       features_train_neg[(train_scheme,tf)]] \n",
    "                                                    for tf in train_transforms])\n",
    "                    train_labels = np.concatenate([np.r_[np.ones((len(features_train_pos[(train_scheme,tf)]), )), \n",
    "                                                        -np.ones((len(features_train_neg[(train_scheme,tf)]), ))]\n",
    "                                                  for tf in train_transforms])\n",
    "\n",
    "                if existing_classifier_id is None:\n",
    "                    clf = train_binary_classifier(train_data, train_labels,\n",
    "                                       alg=feature_classifier_alg, \n",
    "                                       sample_weights=sample_weights)\n",
    "\n",
    "    #                 del train_data, features_train_pos, features_train_neg\n",
    "\n",
    "                    clf_fp = DataManager.get_classifier_filepath(classifier_id=classifier_id, structure=structure)\n",
    "                    save_data(clf, clf_fp)\n",
    "#                     upload_to_s3(clf_fp)\n",
    "                else:\n",
    "                    sys.stderr.write('Load existing classifiers %d\\n' % existing_classifier_id)\n",
    "                    clf = DataManager.load_classifiers(classifier_id=existing_classifier_id)[structure]\n",
    "                \n",
    "                ######################### Compute train metrics #########################\n",
    "                \n",
    "                if extract_train_features:\n",
    "                    train_metrics = compute_classification_metrics(clf.predict_proba(train_data)[:,1], train_labels)\n",
    "                    train_metrics_all_ntrain[n_train][(train_scheme, 'augment' if augment_training else 'no-augment')].append(train_metrics)\n",
    "                \n",
    "                ######################### Test ###############################\n",
    "                \n",
    "                test_transforms = range(1)\n",
    "                features_test_pos = {(test_scheme, tf_variant): \n",
    "                                      [features_dict[(test_scheme, tf_variant)][addr] \n",
    "                                       for addr in addresses_test_pos\n",
    "                                      if addr in features_dict[(test_scheme, tf_variant)]]\n",
    "                                          for tf_variant in test_transforms}\n",
    "                features_test_neg = {(test_scheme, tf_variant): \n",
    "                                      [features_dict[(test_scheme, tf_variant)][addr] \n",
    "                                       for addr in addresses_test_neg\n",
    "                                      if addr in features_dict[(test_scheme, tf_variant)]]\n",
    "                                          for tf_variant in test_transforms}\n",
    "                \n",
    "                test_data = np.concatenate([np.r_[features_test_pos[(test_scheme,tf_variant)], \n",
    "                                  features_test_neg[(test_scheme,tf_variant)]]\n",
    "                                            for tf_variant in train_transforms])\n",
    "                test_labels = np.concatenate([np.r_[np.ones((len(features_test_pos[(test_scheme,tf_variant)]), )), \n",
    "                                     -np.ones((len(features_test_neg[(test_scheme,tf_variant)]), ))]\n",
    "                                            for tf_variant in train_transforms])\n",
    "                test_metrics = compute_classification_metrics(clf.predict_proba(test_data)[:,1], test_labels)\n",
    "    #             print \"acc@0.5 = %.3f, acc@opt = %.3f, opt_thresh = %.3f, auroc = %.3f, auprc = %.3f\" % \\\n",
    "    #             (test_metrics['acc'][0.5], test_metrics['acc'][test_metrics['opt_thresh']], test_metrics['opt_thresh'], test_metrics['auroc'], test_metrics['auprc'])\n",
    "\n",
    "                test_metrics_all_ntrain[n_train][(test_scheme, 'augment' if augment_training else 'no-augment')].append(test_metrics)\n",
    "\n",
    "    train_metrics_all_ntrain.default_factory = None\n",
    "    test_metrics_all_ntrain.default_factory = None\n",
    "    \n",
    "    plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'acc', 0.5), ylabel='Test accuracy@0.5 threshold');\n",
    "    plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'auroc'), ylabel='Area under ROC');\n",
    "\n",
    "    plot_roc_curve(test_metrics_all_ntrain[1000][(test_scheme,\n",
    "                  'no-augment')][0]['fp'], \n",
    "                   test_metrics_all_ntrain[1000][(test_scheme,\n",
    "                  'no-augment')][0]['tp'], \n",
    "                  test_metrics_all_ntrain[1000][(test_scheme,\n",
    "                  'no-augment')][0]['opt_thresh']);\n",
    "    \n",
    "    import uuid\n",
    "\n",
    "    result = {\n",
    "        'n_sections': stack_section_number,\n",
    "        'stain': stack_stain,\n",
    "        'train_stacks': train_stacks,\n",
    "        'test_stacks': test_stacks,\n",
    "        'test_scheme': test_scheme,\n",
    "        'train_scheme': train_scheme,\n",
    "        'train_metrics_all_ntrain': train_metrics_all_ntrain,\n",
    "        'test_metrics_all_ntrain': test_metrics_all_ntrain,\n",
    "        'structure': structure,\n",
    "        'method': method,\n",
    "        'classifier_id': existing_classifier_id if existing_classifier_id is not None else classifier_id\n",
    "    }\n",
    "\n",
    "    create_if_not_exists(ROOT_DIR + '/assessment_results_v4/')\n",
    "    save_pickle(result, ROOT_DIR + '/assessment_results_v4/assessment_result_%s.pkl' % str(uuid.uuid1()).split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9750078756917837"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(extract_one_metric(test_metrics_all_ntrain, 'auroc')[15000][('none','no-augment')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'acc', 0.5), ylabel='Test accuracy@0.5 threshold');\n",
    "plot_result_wrt_ntrain(extract_one_metric(test_metrics_all_ntrain, 'auroc'), ylabel='Area under ROC');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
