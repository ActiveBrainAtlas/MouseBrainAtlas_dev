{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from_fp = 'mousebrainatlas-data/CSHL_data_processed/MD662/\\\n",
    "MD662_prep2_raw_NtbNormalizedAdaptiveInvertedGamma/MD662&661-F100-2017.06.06-21.40.10_MD662_1_0298*'\n",
    "\n",
    "to_fp = '/media/alexn/BstemAtlasDataBackup/CSHL_data_processed/MD662/\\\n",
    "MD662_prep2_raw_NtbNormalizedAdaptiveInvertedGamma/'\n",
    "\n",
    "include_only = '*_0298_*'\n",
    "include_only = 'MD662&661-F100-2017.06.06-21.40.10_MD662_1_0298*'\n",
    "\n",
    "command = 'aws s3 cp --recursive \\\"s3://%(from_fp)s\\\" \\\"%(to_fp)s\\\" \\\n",
    "--exclude \\\"*\\\" --include \\\"%(include)s\\\"'% dict(from_fp=from_fp, to_fp=to_fp, \\\n",
    "                                                 include=include_only)\n",
    "\n",
    "os.system( 'aws s3 cp --recursive \\\"s3://mousebrainatlas-data/CSHL_data_processed/MD662/\\\n",
    "MD662_prep2_raw_NtbNormalizedAdaptiveInvertedGamma/MD662&661-F100-2017.06.06-21.40.10_MD662_1_0298*\\\" \\\"/home/alexn\\\" ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1049, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 1009, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 483, in getmodule\n",
      "    file = getabsfile(object, _filename)\n",
      "  File \"/usr/lib/python2.7/inspect.py\", line 467, in getabsfile\n",
      "    return os.path.normcase(os.path.abspath(_filename))\n",
      "  File \"/usr/lib/python2.7/posixpath.py\", line 364, in abspath\n",
      "    cwd = os.getcwd()\n",
      "OSError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2897\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 1826\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1412\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             )\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/IPython/core/ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m             \u001b[0mstructured_traceback_parts\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mformatted_exception\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructured_traceback_parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.check_output('pwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_fp = 'mousebrainatlas-rawdata/CSHL_volumes/atlasV6/atlasV6_10.0um_scoreVolume/score_volumes/'\n",
    "to_fp = '/media/alexn/BstemAtlasDataBackup/demo/CSHL_volumes/atlasV6/atlasV6_10.0um_scoreVolume/score_volumes/'\n",
    "to_fp = '/home/alexn'\n",
    "include_only = 'atlasV6_10.0um_scoreVolume_12N*'\n",
    "command = 'aws s3 cp --recursive \\\"s3://%(from_fp)s\\\" \\\"%(to_fp)s\\\" --exclude \\\"*\\\" --include \\\"%(include)s\\\"'\\\n",
    "                % dict(from_fp=from_fp, to_fp=to_fp, include=include_only)\n",
    "os.system(command)\n",
    "#execute_command('aws s3 cp --recursive \\\"s3://%(from_fp)s\\\" \\\"%(to_fp)s\\\" --exclude \\\"*\\\" --include \\\"%(include)s\\\"'\\\n",
    "#                % dict(from_fp=from_fp, to_fp=to_fp, include=include_only))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexn/brainDev/src/utilities/utilities2015.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-706d82504182>\", line 18, in <module>\n",
      "    get_ipython().magic(u'matplotlib inline')\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n",
      "    return self.run_line_magic(magic_name, magic_arg_s)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n",
      "    result = fn(*args,**kwargs)\n",
      "  File \"<decorator-gen-105>\", line 2, in matplotlib\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n",
      "    call = lambda f, *a, **k: f(*a, **k)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n",
      "    gui, backend = self.shell.enable_matplotlib(args.gui)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n",
      "    pt.activate_matplotlib(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/pylabtools.py\", line 308, in activate_matplotlib\n",
      "    matplotlib.pyplot.switch_backend(backend)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n",
      "    matplotlib.use(newbackend, warn=False, force=True)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py\", line 1305, in use\n",
      "    reload(sys.modules['matplotlib.backends'])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use('Agg')\n",
      "ENABLE_UPLOAD_S3 is not set, default to False.\n",
      "ENABLE_DOWNLOAD_S3 is not set, default to False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for Precision WorkStation for Alex Newberry\n",
      "{'UCSD001': 0.325}\n",
      "/media/alexn/BstemAtlasDataBackup/ucsd_brain/CSHL_data_processed/UCSD001/UCSD001_cropbox.ini\n",
      "/media/alexn/BstemAtlasDataBackup/ucsd_brain/CSHL_data_processed/UCSD001/UCSD001_cropbox.ini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seems you are using operation INIs to provide cropbox.\n",
      "Seems you are using operation INIs to provide cropbox.\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'MD662'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-706d82504182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MD662'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'testing metadata: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetadata_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_filenames_all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'MD662'"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "\n",
    "# %reload_ext : Reloads an IPython extension by its module name.\n",
    "%reload_ext autoreload\n",
    "# %autoreload 2 : Reloads all modules (except those excluded by %aimport)  \n",
    "#  every time before executing the Python code typed.\n",
    "%autoreload 2\n",
    "\n",
    "# %TEXT : code in this format is called a \"magic function\" \n",
    "#  https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline/43028034\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "#import matplotlib.pyplot as plt # HAS ERR NOW?\n",
    "# Sets backend of matplotlib to the 'inline' backend\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "\n",
    "\n",
    "stack = 'MD662'\n",
    "print('testing metadata: ',metadata_cache['valid_filenames_all'][stack][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download files from S3\n",
    "# Not necessary if files are not on S3\n",
    "! aws s3 cp --recursive --force-glacier-transfer \\\n",
    "    \"s3://mousebrainatlas-rawdata/CSHL_data/MD662\" \"/shared/CSHL_data/MD662\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscillaneous Code\n",
    "#### Function to detect where you are at running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BAD FILE LOCATOR\n",
    "# Automatically tells where in the code you are\n",
    "\n",
    "# 1) raw -> raw_Ntb: extract_channel\n",
    "# 2) raw_Ntb -> thumbnail_Ntb: rescale\n",
    "# 3) thumbnail_Ntb -> thumbnail_NtbNormalized: normalize_intensity\n",
    "# 4) Compute transforms using thumbnail_NtbNormalized: align + compose\n",
    "# 5) Supply prep1_thumbnail_mask\n",
    "# 6) prep1_thumbnail_mask -> thumbnail_mask: warp\n",
    "# 7) raw_Ntb -> raw_NtbNormalizedAdaptiveInvertedGamma: brightness_correction\n",
    "# 8) Compute prep5 (alignedWithMargin) cropping box based on prep1_thumbnail_mask\n",
    "# 9) raw_NtbNormalizedAdaptiveInvertedGamma -> prep5_raw_NtbNormalizedAdaptiveInvertedGamma: align + crop\n",
    "# 10) thumbnail_NtbNormalized -> prep5_thumbnail_NtbNormalized: align + crop\n",
    "# 11) prep5_raw_NtbNormalizedAdaptiveInvertedGamma -> prep5_thumbnail_NtbNormalizedAdaptiveInvertedGamma: rescale\n",
    "# 12) Specify prep2 (alignedBrainstemCrop) cropping box\n",
    "# 13) prep5_raw_NtbNormalizedAdaptiveInvertedGamma -> prep2_raw_NtbNormalizedAdaptiveInvertedGamma: crop\n",
    "# 14) prep2_raw_NtbNormalizedAdaptiveInvertedGamma -> prep2_raw_NtbNormalizedAdaptiveInvertedGammaJpeg: compress_jpeg\n",
    "\n",
    "# np.setdiff1d(list1,list2) # Get difference between two lists\n",
    "# metadata_cache['valid_filenames_all'][stack]  # To get all valid filenames\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "\n",
    "def where_am_i():\n",
    "    '''\n",
    "    where_am_i will iterate through each of the 14 steps of preprocessing, checking that ALL output files \n",
    "    have been generated sucessfully. For any stage where files are missing or corrupted, this function will \n",
    "    return a list of the filepaths & filenames of the images that are not in the output.\n",
    "    where_am_i() returns [bad_fp_list, bad_file_list].\n",
    "    '''\n",
    "    \n",
    "    # Throws an error if you don't set Max Pixels to around 2 Billion (yikes)\n",
    "    Image.MAX_IMAGE_PIXELS = 2000000000\n",
    "    # Iterates through every possible filename as defined in metadata_cache\n",
    "    #                                           stack must be defined\n",
    "    bad_fp_list = [] # Full filePaths\n",
    "    bad_file_list = [] # FileNames\n",
    "    \n",
    "    #   Dic = {'step#': [prep_id, resol, version]}\n",
    "    stepDic = {'1':[None,'raw','Ntb'], '2':[None,'thumbnail','Ntb'], '3':[None,'thumbnail','NtbNormalized'],\\\n",
    "               '4':[None,'???','???'], '5':[1,'thumbnail','mask'], '6':[None,'???','???'], \\\n",
    "               '7':[None,'raw','NtbNormalizedAdaptiveInvertedGamma'], '8':[5,'thumbnail','mask'], \\\n",
    "               '9':[5,'raw','NtbNormalizedAdaptiveInvertedGamma'], '10':[5,'thumbnail','NtbNormalized'], \\\n",
    "               '11':[5,'thumbnail','NtbNormalizedAdaptiveInvertedGamma']}\n",
    "\n",
    "    \n",
    "    # For checking each individual step of the pipeline as outlined above\n",
    "    for step in range (2,12):\n",
    "        stepStr = str(step)\n",
    "        stepCompleted = True\n",
    "        \n",
    "        print 'Checking for completion of step '+stepStr\n",
    "        t = time.time()\n",
    "        \n",
    "        if stepDic[stepStr][1]=='???':\n",
    "                stepCompleted = False\n",
    "                print 'Output files unknown. No idea.'\n",
    "                print ''\n",
    "                continue\n",
    "        \n",
    "        for fn in metadata_cache['valid_filenames_all'][stack]:\n",
    "            \n",
    "            # Record the filepath from every filename + info from the \"step dictionary\"\n",
    "            fp = DataManager.get_image_filepath_v2(stack=stack, prep_id=stepDic[stepStr][0], \\\n",
    "                                     resol=stepDic[stepStr][1], version=stepDic[stepStr][2], fn=fn)\n",
    "            \n",
    "            fp_to_test = fp\n",
    "            try:\n",
    "                img = Image.open(fp_to_test) # open the image file\n",
    "                img.verify() # verify that it is, in fact an uncorruted image\n",
    "            except (IOError, SyntaxError) as e:\n",
    "               # print('Bad file:', fp_to_test) # Print out the names of corrupt files\n",
    "                bad_fp_list.append(fp_to_test) # Add bad filepaths to bad_filepath_list\n",
    "                bad_file_list.append(fn) # Add bad files to bad_file_list\n",
    "                stepCompleted = False\n",
    "        #metadata_cache['valid_filenames_all']['MD662']\n",
    "        \n",
    "        if stepCompleted:\n",
    "            print 'Step '+stepStr+' completed!'\n",
    "            print 'done in', time.time() - t, 'seconds' \n",
    "            print ''\n",
    "        else:\n",
    "            print '*****     Bad files found!     *****\\n'\n",
    "            print 'Step '+stepStr+' NOT complete. Rerun necessary parts of the code'\n",
    "            print '`bad_fp_list` and `bad_file_list` created, each contain info on each incomplete files.'\n",
    "            print 'done in', time.time() - t, 'seconds' \n",
    "            print ''\n",
    "            if stepDic[stepStr][0] is None:\n",
    "                print '`'+stack+'_'+stepDic[stepStr][1]+'_'+\\\n",
    "                        stepDic[stepStr][2]+'` was not generated properly'\n",
    "            else:\n",
    "                print '`'+stack+'_prep'+str(stepDic[stepStr][0])+'_'+stepDic[stepStr][1]+'_'+\\\n",
    "                        stepDic[stepStr][2]+'` was not generated properly'\n",
    "            break\n",
    "            #continue\n",
    "    return bad_fp_list, bad_file_list\n",
    "\n",
    "#print where_am_i.__doc__\n",
    "[bad_fp_list, bad_file_list] = where_am_i()\n",
    "\n",
    "print len(bad_file_list),' bad files'\n",
    "print 'bad_fp_list and bad_file_list contain all of the problematic file names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify raw file locations\n",
    "\n",
    "# replaces running \"initialize.py <input_spec_filepath>\"\n",
    "\n",
    "# The set of dirs where we should search for image files.\n",
    "raw_data_dirs = \\\n",
    "{(None, 'raw'): '/media/alexn/BstemAtlasDataBackup/MD662/',\n",
    "(None, 'down32'): '/media/alexn/BstemAtlasDataBackup/MD662/'}\n",
    "\n",
    "# Specifies how to extract image name from file path.\n",
    "# The first group returned by re.search is image_name.\n",
    "input_image_filename_to_imagename_re_pattern_mapping = \\\n",
    "{(None, 'raw'): \\\n",
    " '/media/alexn/BstemAtlasDataBackup/MD662/(.*)_lossless.jp2',\n",
    " (None, 'down32'): \\\n",
    "  '/media/alexn/BstemAtlasDataBackup/MD662/(.*).png', # Check raw data, may NOT be a .png file\n",
    "}\n",
    "\n",
    "# Check that every raw file has all 3 channels properly\n",
    "image_names_all_data_dirs_flattened = set([])\n",
    "image_names_all_data_dirs = {}\n",
    "for vr, data_dir in raw_data_dirs.iteritems():\n",
    "    if data_dir is None: continue\n",
    "    image_names = set([])\n",
    "    if vr in input_image_filename_to_imagename_re_pattern_mapping:\n",
    "        for fn in os.listdir(data_dir):\n",
    "            g = re.search(input_image_filename_to_imagename_re_pattern_mapping[vr], os.path.join(data_dir, fn))\n",
    "            if g is not None:\n",
    "                img_name = g.groups()[0]\n",
    "                image_names.add(img_name)\n",
    "                image_names_all_data_dirs_flattened.add(img_name)\n",
    "    image_names_all_data_dirs[vr] = image_names\n",
    "    \n",
    "# Make sure the every image has all three channels.\n",
    "for vr, img_names in image_names_all_data_dirs.iteritems():\n",
    "    print vr, 'missing:'\n",
    "    print image_names_all_data_dirs_flattened - img_names\n",
    "    print \n",
    "    \n",
    "# set([]) == Good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other instructions \n",
    "- You need to specify in metadata.py relevant filepaths. You need to set certain environment variables. User will need to specify stack name as well.\n",
    "- If thumbnails (downsampled 32x) aren't provided run: resize.py [in_fp_map] [out_fp_map] 0.03125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify input filepaths [From User Guide]\n",
    "Create a JSON file that describes the image file paths as described in preprocessing.md: https://github.com/ActiveBrainAtlas/MouseBrainAtlas/blob/master/doc/User%20Manuals/user_guide_pages/Preprocessing.md\n",
    "\n",
    "Then run `initialize.py <input_spec_filepath>`\n",
    "\n",
    "thumbnail_NtbNormalized ->\n",
    "\n",
    "                 -> <stack>_sorted_filenames.txt\n",
    " \n",
    "                 -> <stack>_thumbnail_Ntb/\n",
    " \n",
    "                 -> <stack>_raw_Ntb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "### raw (.jp2) -> raw_Ntb (.tif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = '/media/alexn/BstemAtlasDataBackup/MD662/'\n",
    "output_dir = create_if_not_exists(DataManager.get_image_dir_v2(stack=stack, prep_id=None, resol='raw'))\n",
    "\n",
    "# INPUT TEST\n",
    "in_list = [os.path.join(in_dir, img_name + '_lossless.jp2') \n",
    "                                       for img_name in list(image_names_all_data_dirs_flattened)]\n",
    "# OUTPUT TEST\n",
    "out_list = [DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \n",
    "                                        resol='raw', version=None, fn=img_name) \n",
    "                                        for img_name in list(image_names_all_data_dirs_flattened)]\n",
    "\n",
    "print('first input file: ',in_list[0])\n",
    "print('first output file: ',out_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiple core processing on every individual file (takes about 4 minutes each)\n",
    "# In total will take about 30 hours\n",
    "# Creates new files at /CSHL_data_processed/MD662/MD662_raw/*_raw.tif\n",
    "t = time.time()\n",
    "\n",
    "run_distributed('export LD_LIBRARY_PATH=%(kdu_dir)s:$LD_LIBRARY_PATH; %(kdu_bin)s -i \\\"%%(in_fp)s\\\" -o \\\"%%(out_fp)s\\\"' % \\\n",
    "                {'kdu_bin': KDU_EXPAND_BIN, 'kdu_dir': os.path.dirname(KDU_EXPAND_BIN)},\n",
    "                kwargs_list={'in_fp': [os.path.join(in_dir, img_name + '_lossless.jp2') \n",
    "                                       for img_name in list(image_names_all_data_dirs_flattened)], \n",
    "                             'out_fp': [DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \n",
    "                                        resol='raw', version=None, fn=img_name) \n",
    "                                        for img_name in list(image_names_all_data_dirs_flattened)]},\n",
    "                argument_type='single',\n",
    "                jobs_per_node=1, # Use single process\n",
    "                local_only=True, # Run local\n",
    "                use_aws=False)   # Run local\n",
    "print 'done in', time.time() - t, 'seconds' # 2252 seconds full stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2\n",
    "### raw_Ntb -> thumbnail_Ntb\n",
    "rescale\n",
    "\n",
    "# & STEP 3\n",
    "### thumbnail_Ntb -> thumbnail_NtbNormalized\n",
    "normalize_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: qhost: not found\r\n"
     ]
    }
   ],
   "source": [
    "! qhost | awk 'NR >= 4 { print $1 }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm -f /media/alexn/BstemAtlasDataBackup/ucsd_brain/mousebrainatlas_tmp/stderr_*; rm -f /media/alexn/BstemAtlasDataBackup/ucsd_brain/mousebrainatlas_tmp/stdout_*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qhost | awk 'NR >= 4 { print $1 }'\n",
      "\n",
      "\n",
      "NODE LIST LENGTH IS 0. NO HOSTS AVAILABLE\n",
      "finished generating raw_Ntb files\n",
      "done in 0.40748000145 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "return code: 0\n",
      "0 nodes available.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "stack='UCSD001'\n",
    "image_names_all_data_dirs_flattened = ['UCSD001_slide001_2018_09_03-S1','UCSD001_slide002_2018_09_03-S1']\n",
    "# raw -> raw_Ntb\n",
    "# Multiple core processing on every individual file (takes about 4 minutes each)\n",
    "# In total will take about 20 hours\n",
    "# Creates new files at CSHL_data_processed/MD662/MD662_raw_Ntb/*_raw_Ntb.tif\n",
    "t = time.time()\n",
    "run_distributed('convert \\\"%(in_fp)s\\\" -channel B -separate \\\"%(out_fp)s\\\"',\n",
    "                kwargs_list={'in_fp': [DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \n",
    "                                        resol='raw', version=None, fn=img_name) \n",
    "                                       for img_name in list(image_names_all_data_dirs_flattened)], \n",
    "                             'out_fp': [DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \n",
    "                                        resol='raw', version='Ntb', fn=img_name) \n",
    "                                       for img_name in list(image_names_all_data_dirs_flattened)]},\n",
    "                argument_type='single',\n",
    "                jobs_per_node=1,\n",
    "                local_only=False,\n",
    "               use_aws=False)\n",
    "print('finished generating raw_Ntb files')\n",
    "print 'done in', time.time() - t, 'seconds' # 2252 seconds full stack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_if_not_exists(DataManager.get_image_dir_v2(stack=stack, prep_id=None, resol='raw', version='Ntb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# raw -> raw_Ntb\n",
    "# Multiple core processing on every individual file (takes about 4 minutes each)\n",
    "# In total will take about 20 hours\n",
    "# Creates new files at CSHL_data_processed/MD662/MD662_raw_Ntb/*_raw_Ntb.tif\n",
    "t = time.time()\n",
    "run_distributed('convert \\\"%(in_fp)s\\\" -channel B -separate \\\"%(out_fp)s\\\"',\n",
    "                kwargs_list={'in_fp': [DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \n",
    "                                        resol='raw', version=None, fn=img_name) \n",
    "                                       for img_name in list(image_names_all_data_dirs_flattened)], \n",
    "                             'out_fp': [DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \n",
    "                                        resol='raw', version='Ntb', fn=img_name) \n",
    "                                        for img_name in list(image_names_all_data_dirs_flattened)]},\n",
    "                argument_type='single',\n",
    "                jobs_per_node=1,\n",
    "                local_only=True,\n",
    "               use_aws=False)\n",
    "print('finished generating raw_Ntb files')\n",
    "print 'done in', time.time() - t, 'seconds' # 2252 seconds full stack\n",
    "\n",
    "thumbnail_downscale_factor = 32\n",
    "tb_resol = 'thumbnail'\n",
    "\n",
    "# Takes about 1 minute per file, in total about 8-10 hours\n",
    "\n",
    "# Will create new files at the filepaths /CSHL_data_processed/MD662/MD662_thumbnail_Ntb/*_thumbnail_Ntb.tif\n",
    "#  and /CSHL_data_processed/MD662/MD662_thumbnail_NtbNormalized/*_thumbnail_NtbNormalized.tif\n",
    "# Only 108 files in each directory though?\n",
    "i = 0\n",
    "\n",
    "for img_name in metadata_cache['valid_filenames_all'][stack]:\n",
    "    i = i+1\n",
    "    print '\\n\\n'+img_name+'\\n'\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    in_fp = DataManager.get_image_filepath_v2(stack=stack, prep_id=None, resol='raw', version='Ntb', \\\n",
    "                                              fn=img_name)\n",
    "    out_fp = DataManager.get_image_filepath_v2(stack=stack, prep_id=None, resol=tb_resol, version='Ntb', \\\n",
    "                                              fn=img_name)\n",
    "    create_parent_dir_if_not_exists(out_fp)\n",
    "    \n",
    "  #  if os.path.isfile(out_fp):\n",
    "  #      print('SKIPPING NeurotraceB: '+out_fp)\n",
    "  #      #continue\n",
    "  #  else:\n",
    "    \n",
    "    try:\n",
    "        img = imread(in_fp)\n",
    "    except IndexError:\n",
    "        print('Problematic file detected\\n\\n'+out_fp+'\\n\\n')\n",
    "        \n",
    "    print(out_fp)\n",
    "    img_tb = img[::thumbnail_downscale_factor, ::thumbnail_downscale_factor]\n",
    "    imsave(out_fp, img_tb)\n",
    "    \n",
    "    \n",
    "    # Alternative: ImageMagick introduces an artificial noisy stripe in the output image.\n",
    "#     cmd = 'convert %(in_fp)s -scale 3.125%% %(out_fp)s' % {'in_fp': in_fp, 'out_fp': out_fp}\n",
    "#     execute_command(cmd)\n",
    "        \n",
    "    sys.stderr.write(\"Rescale: %.2f seconds.\\n\" % (time.time() - t)) # ~20s / image\n",
    "    \n",
    "    t = time.time()\n",
    "\n",
    "    in_fp = DataManager.get_image_filepath_v2(stack=stack, prep_id=None, resol=tb_resol, version='Ntb', \\\n",
    "                                              fn=img_name)\n",
    "    out_fp = DataManager.get_image_filepath_v2(stack=stack, prep_id=None, resol=tb_resol, version='NtbNormalized',\\\n",
    "                                               fn=img_name)\n",
    "    create_parent_dir_if_not_exists(out_fp)\n",
    "    \n",
    "  #  if os.path.isfile(out_fp):\n",
    "  #      print('SKIPPING Ntb Normalized: '+out_fp)\n",
    "  #      continue\n",
    "  #  else:\n",
    "    print(out_fp)\n",
    "    cmd = \"\"\"convert \"%(in_fp)s\" -normalize -depth 8 \"%(out_fp)s\" \"\"\" % {'in_fp': in_fp, 'out_fp': out_fp}\n",
    "    execute_command(cmd)\n",
    "  #  try:\n",
    "  #      img = imread(in_fp)\n",
    "  #  except IndexError:\n",
    "  #      print('Problematic file detected\\n\\n'+out_fp+'\\n\\n')\n",
    "    \n",
    "    \n",
    "    sys.stderr.write(\"Intensity normalize: %.2f seconds.\\n\" % (time.time() - t))\n",
    "print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4\n",
    "### Compute transforms using thumbnail_NtbNormalized\n",
    "align + compose\n",
    "\n",
    "#### Generally how it works:\n",
    "- Generate Elastix output from thumbnail_NtbNormalized\n",
    "    - These are pairwise transformations for the files\n",
    "        - Saved in /elastix_output/\n",
    "    - Roughly 3% will fail and will need to be run through. Use GUI to fix\n",
    "        - Saved in /custom_transform/\n",
    "- Select an anchor file to unite all pairwise transformations\n",
    "    - /gui/preprocess_tool_v3.py\n",
    "        - Anchor.txt is created with name of anchor file\n",
    "    - Transformation matrices generated for each file relative to anchor\n",
    "        - Saved in transformTo_[anchorName].pkl\n",
    "    - Generate /prep1_thumbnail_normalized/ images. May be padded with white\n",
    "        - prep1_thumbnail_normalized files used as inputs to \"Active Contour Algorithm\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, sections_to_filenames = DataManager.load_sorted_filenames(stack=stack, redownload=True) \n",
    "\n",
    "tb_fmt = 'tif'\n",
    "version = 'NtbNormalized'\n",
    "valid_filenames = metadata_cache['valid_filenames_all'][stack]\n",
    "# Note that this could be the human-corrected version, in which case the transforms may not exist.\n",
    "valid_filenames = [fn for fn in sections_to_filenames.values() if not is_invalid(fn=fn)]\n",
    "\n",
    "script = os.path.join(REPO_DIR, 'preprocess', 'align_consecutive_v2.py')\n",
    "input_dir = DataManager.get_image_dir_v2(stack=stack, prep_id=None, version=version, resol='thumbnail')    \n",
    "output_dir = create_if_not_exists(os.path.join(THUMBNAIL_DATA_DIR, stack, stack + '_elastix_output'))\n",
    "print('Input: ',input_dir)\n",
    "print('Output: ',output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "print 'Align...'\n",
    "\n",
    "run_distributed(\"%(script)s %(stack)s \\\"%(input_dir)s\\\" \\\"%(output_dir)s\\\" \\'%%(kwargs_str)s\\' %(fmt)s -p \\\n",
    "%(param_fp)s -r\" % \\\n",
    "                {'script': script,\n",
    "                'stack': stack,\n",
    "                'input_dir': input_dir,\n",
    "                'output_dir': output_dir,\n",
    "                'fmt': tb_fmt,\n",
    "                 'param_fp': '/home/yuncong/Brain/preprocess/parameters/Parameters_Rigid_MutualInfo_noNumberOfSpatialSamples_4000Iters.txt'\n",
    "                },\n",
    "                kwargs_list=[{'prev_fn': valid_filenames[i-1] + '_thumbnail_' + version, \n",
    "                              'curr_fn': valid_filenames[i] + '_thumbnail_' + version,\n",
    "                             'prev_sn': valid_filenames[i-1] ,\n",
    "                             'curr_sn': valid_filenames[i] } \n",
    "                             for i in range(1, len(valid_filenames))],\n",
    "                argument_type='list',\n",
    "                jobs_per_node=8,\n",
    "               local_only=True)\n",
    "\n",
    "# wait_qsub_complete()\n",
    "\n",
    "print 'done in', time.time() - t, 'seconds' # 2252 seconds full stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RUN GUI TO CHECK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5\n",
    "### Supply prep1_thumbnail_mask \n",
    "^ Need to clarify with Yuncong HOW to do this\n",
    "\n",
    "#### Also compose alignments\n",
    "-> [stackName]_transformsTo_[anchorFilename].pkl\n",
    "\n",
    "thumbnail_NtbNormalized ->\n",
    "[stackName]\\_transformsTo\\_[anchorFilename].pkl -> prep1_thumbnail_NtbNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-87085713460d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manchor_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_anchor_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0manchor_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_filenames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'anchor_idx ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREPO_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'preprocess'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compose_transform_thumbnail_v2.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataManager' is not defined"
     ]
    }
   ],
   "source": [
    "anchor_fn = DataManager.load_anchor_filename(stack=stack)\n",
    "anchor_idx = valid_filenames.index(anchor_fn)\n",
    "print 'anchor_idx =', anchor_idx\n",
    "\n",
    "script = os.path.join(REPO_DIR, 'preprocess', 'compose_transform_thumbnail_v2.py')\n",
    "input_dir = os.path.join(DATA_DIR, stack, stack + '_elastix_output')\n",
    "output_fp = os.path.join(DATA_DIR, stack, '%(stack)s_transformsTo_%(anchor_fn)s.pkl' % \\\n",
    "                         dict(stack=stack, anchor_fn=anchor_fn))\n",
    "\n",
    "print('Script: ',script,'\\n')\n",
    "print('Input: ',input_dir)\n",
    "print('Output: ',output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -> [stackName]_transformsTo_[anchorFilename].pkl\n",
    "t = time.time()\n",
    "print 'Composing transform...'\n",
    "\n",
    "run_distributed(\"%(script)s %(stack)s \\\"%(input_dir)s\\\" \\'%%(kwargs_str)s\\' %(anchor_idx)d \\\"%(output_fp)s\\\"\" % \\\n",
    "            {'stack': stack,\n",
    "            'script': script,\n",
    "            'input_dir': input_dir,\n",
    "            'anchor_idx': anchor_idx,\n",
    "            'output_fp': output_fp},\n",
    "            kwargs_list=[{'filenames': metadata_cache['valid_filenames_all'][stack]}],\n",
    "            argument_type='list',\n",
    "               local_only=True)\n",
    "\n",
    "# wait_qsub_complete()\n",
    "\n",
    "print 'done in', time.time() - t, 'seconds' # 20 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "print 'Warping...'\n",
    "\n",
    "transforms_to_anchor = DataManager.load_transforms(stack=stack, downsample_factor=32, \\\n",
    "                                                   use_inverse=False, anchor_fn=anchor_fn)\n",
    "# useful for alternatively stained stacks where bg varies depending on stain on each section\n",
    "if pad_bg_color == 'auto': \n",
    "    run_distributed('%(script)s %(stack)s \\\"%%(input_fp)s\\\" \\\"%%(output_fp)s\\\" %%(transform)s \\\n",
    "    thumbnail 0 0 2000 1500 %%(pad_bg_color)s' % \\\n",
    "                    {'script': script,\n",
    "                    'stack': stack,\n",
    "                    },\n",
    "                    kwargs_list=[{'transform': ','.join(map(str, transforms_to_anchor[fn].flatten())),\n",
    "                                'input_fp': DataManager.get_image_filepath_v2(stack=stack, fn=fn, \\\n",
    "                                                            prep_id=None, version=version, resol='thumbnail'),\n",
    "                                  'output_fp': DataManager.get_image_filepath_v2(stack=stack, fn=fn,\\\n",
    "                                                            prep_id=prep_id, version=version, resol='thumbnail'),\n",
    "                                'pad_bg_color': 'black' if fn.split('-')[1][0] == 'F' else 'white'}\n",
    "                                for fn in metadata_cache['valid_filenames_all'][stack]],\n",
    "                    argument_type='single',\n",
    "                   jobs_per_node=8,\n",
    "                   local_only=True)\n",
    "else:\n",
    "    run_distributed('%(script)s %(stack)s \\\"%%(input_fp)s\\\" \\\"%%(output_fp)s\\\" %%(transform)s \\\n",
    "    thumbnail 0 0 2000 1500 %(pad_bg_color)s' % \\\n",
    "                    {'script': script,\n",
    "                    'stack': stack,\n",
    "                    'pad_bg_color': pad_bg_color},\n",
    "                    kwargs_list=[{'transform': ','.join(map(str, transforms_to_anchor[fn].flatten())),\n",
    "                                'input_fp': DataManager.get_image_filepath_v2(stack=stack, fn=fn, \\\n",
    "                                                            prep_id=None, version=version, resol='thumbnail'),\n",
    "                                  'output_fp': DataManager.get_image_filepath_v2(stack=stack, fn=fn, \\\n",
    "                                                            prep_id=prep_id, version=version, resol='thumbnail'),\n",
    "                                 }\n",
    "                                for fn in metadata_cache['valid_filenames_all'][stack]],\n",
    "                    argument_type='single',\n",
    "                   jobs_per_node=8,\n",
    "                   local_only=True)\n",
    "\n",
    "# wait_qsub_complete()\n",
    "    \n",
    "print 'done in', time.time() - t, 'seconds' # 300 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6\n",
    "### prep1_thumbnail_mask -> thumbnail_mask\n",
    "warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[\n",
    "### Next step is to run the \"Active Conour Algorithm\", the mask generation step.\n",
    "prep1_thumbnail_normalized -> prep1_thumbnail_mask\n",
    "### For now, I am skipping this step. Y suggested I download masks from S3 directly and skip to the alignment process.\n",
    "Skipping Mask Generation. Downloading masks.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download_from_s3(os.path.join('CSHL_data_processed', stack, stack + '_prep1_thumbnail_mask'), \n",
    "                 is_dir=True, local_root=DATA_ROOTDIR, redownload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7\n",
    "### raw_Ntb -> raw_NtbNormalizedAdaptiveInvertedGamma\n",
    "brightness_correction\n",
    "\n",
    "(Requires *_raw_normalizedFloatMap.bp' for each file)\n",
    "generate raw_mask -> generate STACK_intensity_normalization_results/normalizedFloatMap/ files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.exposure import rescale_intensity, adjust_gamma\n",
    "\n",
    "# Generates the raw_mask\n",
    "for section in metadata_cache['valid_sections_all'][stack]:\n",
    "    \n",
    "    print \"Section\", section\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    img = DataManager.load_image_v2(stack=stack, prep_id=None, section=section, version='Ntb', resol='raw')\n",
    "\n",
    "    sys.stderr.write('Load image: %.2f seconds.\\n' % (time.time() - t))\n",
    "\n",
    "    t = time.time()\n",
    "    tb_mask = DataManager.load_thumbnail_mask_v3(stack=stack, prep_id=None, section=section)\n",
    "#     raw_mask = rescale_by_resampling(tb_mask, new_shape=(img.shape[1], img.shape[0]))\n",
    "    raw_mask = resize(tb_mask, img.shape) > .5\n",
    "    \n",
    "    save_data(raw_mask, \n",
    "          DataManager.get_image_filepath_v2(stack=stack, prep_id=None, section=section, version='mask', resol='raw', ext='bp'), \n",
    "          upload_s3=False)\n",
    "    \n",
    "    sys.stderr.write('Rescale mask: %.2f seconds.\\n' % (time.time() - t))\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    mean_std_all_regions = []\n",
    "    cx_cy_all_regions = []\n",
    "    region_size = 5000\n",
    "    region_spacing = 3000\n",
    "    for cx in range(0, img.shape[1], region_spacing):\n",
    "        for cy in range(0, img.shape[0], region_spacing):\n",
    "            region = img[max(cy-region_size/2, 0):min(cy+region_size/2+1, img.shape[0]-1), \n",
    "                         max(cx-region_size/2, 0):min(cx+region_size/2+1, img.shape[1]-1)]\n",
    "            region_mask = raw_mask[max(cy-region_size/2, 0):min(cy+region_size/2+1, img.shape[0]-1), \n",
    "                                   max(cx-region_size/2, 0):min(cx+region_size/2+1, img.shape[1]-1)]\n",
    "            if np.count_nonzero(region_mask) == 0:\n",
    "                continue\n",
    "            mean_std_all_regions.append((region[region_mask].mean(), region[region_mask].std()))\n",
    "            cx_cy_all_regions.append((cx, cy))\n",
    "            \n",
    "    sys.stderr.write('Compute mean/std for sample regions: %.2f seconds.\\n' % (time.time() - t))\n",
    "    \n",
    "    t = time.time()\n",
    "    mean_map = resample_scoremap(sparse_scores=np.array(mean_std_all_regions)[:,0], \n",
    "                             sample_locations=cx_cy_all_regions,\n",
    "                             gridspec=(region_size, region_spacing, img.shape[1], img.shape[0], (0,0)),\n",
    "                            downscale=4, \n",
    "                                 interpolation_order=2)\n",
    "\n",
    "    sys.stderr.write('Interpolate mean map: %.2f seconds.\\n' % (time.time() - t)) #10s\n",
    "\n",
    "    t = time.time()\n",
    "    mean_map = rescale_by_resampling(mean_map, new_shape=(img.shape[1], img.shape[0]))\n",
    "    sys.stderr.write('Scale up mean map: %.2f seconds.\\n' % (time.time() - t)) #30s\n",
    "\n",
    "    t = time.time()\n",
    "    std_map = resample_scoremap(sparse_scores=np.array(mean_std_all_regions)[:,1], \n",
    "                             sample_locations=cx_cy_all_regions,\n",
    "                             gridspec=(region_size, region_spacing, img.shape[1], img.shape[0], (0,0)),\n",
    "                            downscale=4,\n",
    "                               interpolation_order=2)\n",
    "    sys.stderr.write('Interpolate std map: %.2f seconds.\\n' % (time.time() - t)) #10s\n",
    "\n",
    "    t = time.time()\n",
    "    std_map = rescale_by_resampling(std_map, new_shape=(img.shape[1], img.shape[0]))\n",
    "    sys.stderr.write('Scale up std map: %.2f seconds.\\n' % (time.time() - t)) #30s\n",
    "    \n",
    "    # Save mean/std results.\n",
    "    \n",
    "    fp = DataManager.get_intensity_normalization_result_filepath(what='region_centers', stack=stack, section=section)\n",
    "    create_parent_dir_if_not_exists(fp)    \n",
    "    np.savetxt(fp, cx_cy_all_regions)\n",
    "    \n",
    "    fp = DataManager.get_intensity_normalization_result_filepath(what='mean_std_all_regions', stack=stack, section=section)\n",
    "    create_parent_dir_if_not_exists(fp)\n",
    "    np.savetxt(fp, mean_std_all_regions)\n",
    "    \n",
    "    fp = DataManager.get_intensity_normalization_result_filepath(what='mean_map', stack=stack, section=section)\n",
    "    create_parent_dir_if_not_exists(fp)\n",
    "    bp.pack_ndarray_file(mean_map.astype(np.float16), fp)\n",
    "    \n",
    "    fp = DataManager.get_intensity_normalization_result_filepath(what='std_map', stack=stack, section=section)\n",
    "    create_parent_dir_if_not_exists(fp)\n",
    "    bp.pack_ndarray_file(std_map.astype(np.float16), fp)\n",
    "\n",
    "    # Export normalized image.\n",
    "    \n",
    "    t = time.time()\n",
    "    raw_mask = raw_mask & (std_map > 0)\n",
    "    img_normalized = np.zeros(img.shape, np.float32)\n",
    "    img_normalized[raw_mask] = (img[raw_mask] - mean_map[raw_mask]) / std_map[raw_mask]\n",
    "    sys.stderr.write('Normalize: %.2f seconds.\\n' % (time.time() - t)) #30s\n",
    "\n",
    "    t = time.time()\n",
    "    # FIX THIS! THIS only save uint16, not float16. Need to save as bp instead.\n",
    "#     img_fp = DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \\\n",
    "#version='NtbNormalizedFloat', resol='down8', section=section, )\n",
    "#     create_parent_dir_if_not_exists(img_fp)\n",
    "#     imsave(img_fp, img_normalized[::8, ::8].astype(np.float16))\n",
    "    save_data(img_normalized.astype(np.float16), \n",
    "              DataManager.get_intensity_normalization_result_filepath(what='normalized_float_map', stack=stack, section=section),\n",
    "             upload_s3=False)\n",
    "    sys.stderr.write('Save float version: %.2f seconds.\\n' % (time.time() - t)) #30s\n",
    "    \n",
    "    \n",
    "    # Export histogram.\n",
    "    \n",
    "    plt.hist(img_normalized[raw_mask].flatten(), bins=100, log=True);\n",
    "    fp = DataManager.get_intensity_normalization_result_filepath(what='float_histogram_png', stack=stack, section=section)\n",
    "    create_parent_dir_if_not_exists(fp)\n",
    "    plt.savefig(fp)\n",
    "    plt.close();\n",
    "    \n",
    "#     hist_fp = DataManager.get_intensity_normalization_result_filepath(what='float_histogram', stack=stack, section=section)\n",
    "#     create_parent_dir_if_not_exists(hist_fp)\n",
    "    \n",
    "#     hist, bin_edges = np.histogram(img_normalized[valid_mask].flatten(), bins=np.arange(0,201,5));\n",
    "\n",
    "#     plt.bar(bin_edges[:-1], np.log(hist));\n",
    "#     plt.xticks(np.arange(0, 200, 20), np.arange(0, 200, 20));\n",
    "#     plt.xlabel('Normalized pixel value (float)');\n",
    "#     plt.title(metadata_cache['sections_to_filenames'][stack][section])\n",
    "\n",
    "#     plt.savefig(hist_fp)\n",
    "#     plt.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5-10 seconds per section\n",
    "for section in metadata_cache['valid_sections_all'][stack]:\n",
    "        \n",
    "    print section\n",
    "    \n",
    "    raw_mask = load_data(DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \\\n",
    "                                     section=section, version='mask', resol='raw', ext='bp'),download_s3=False)\n",
    "    \n",
    "    img_normalized = load_data(\n",
    "              DataManager.get_intensity_normalization_result_filepath(what='normalized_float_map', \\\n",
    "                                     stack=stack, section=section), download_s3=False)\n",
    "        \n",
    "    q = img_normalized[raw_mask]\n",
    "    \n",
    "    save_data(np.percentile(q, range(101)), \n",
    "              DataManager.get_intensity_normalization_result_filepath(what='float_percentiles', \\\n",
    "                                                    stack=stack, section=section), upload_s3=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma_map = img_as_ubyte(adjust_gamma(np.arange(0, 256, 1) / 255., 8.))\n",
    "# 10-30 seconds per section\n",
    "for section in metadata_cache['valid_sections_all'][stack]:\n",
    "\n",
    "    print section\n",
    "    \n",
    "    img_normalized = load_data(\n",
    "              DataManager.get_intensity_normalization_result_filepath(what='normalized_float_map', \\\n",
    "                                                        stack=stack, section=section), download_s3=False)\n",
    "        \n",
    "    t = time.time()\n",
    "    img_normalized_uint8 = rescale_intensity_v2(img_normalized, -2., 50.)\n",
    "    sys.stderr.write('Rescale to uint8: %.2f seconds.\\n' % (time.time() - t))\n",
    "\n",
    "    t = time.time()\n",
    "    raw_mask = load_data(DataManager.get_image_filepath_v2(stack=stack, prep_id=None, section=section, \\\n",
    "                                                    version='mask', resol='raw', ext='bp'), download_s3=False)\n",
    "    img_normalized_uint8[~raw_mask] = 0\n",
    "    sys.stderr.write('Load mask: %.2f seconds.\\n' % (time.time() - t))\n",
    "    \n",
    "#     t = time.time()\n",
    "#     save_data(img_normalized_uint8, DataManager.get_image_filepath_v2(stack=stack, prep_id=None, \\\n",
    "#section=section, version='NtbNormalizedAdaptive', resol='raw'),\n",
    "#              upload_s3=False)\n",
    "#     sys.stderr.write('Save uint8 version: %.2f seconds.\\n' % (time.time() - t))\n",
    "    \n",
    "#     img_normalized_uint8 = \\\n",
    "#     DataManager.load_image_v2(stack=stack, prep_id=None, section=section, version='NtbNormalizedAdaptive',\\\n",
    "#resol='raw')\n",
    "    img = 255 - img_normalized_uint8\n",
    "    save_data(gamma_map[img], \n",
    "              DataManager.get_image_filepath_v2(stack=stack, prep_id=None, section=section, \\\n",
    "                                    version='NtbNormalizedAdaptiveInvertedGamma', resol='raw'), upload_s3=False)\n",
    "# outputs the *_raw_NtbNormalizedAdaptiveInvertedGamma files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8\n",
    "### prep1 -> prep5\n",
    "Compute prep5 (alignedWithMargin) cropping box based on prep1_thumbnail_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 9\n",
    "### raw_NtbNormalizedAdaptiveInvertedGamma -> prep5_raw_NtbNormalizedAdaptiveInvertedGamma\n",
    "align + crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# roughly 1 second per slice\n",
    "# Generates *_prep5_raw_NtbNormalizedAdaptiveInvertedGamma\n",
    "for stack in ['MD662']:\n",
    "\n",
    "    alignedWithMargin_xmin, alignedWithMargin_xmax,\\\n",
    "    alignedWithMargin_ymin, alignedWithMargin_ymax = DataManager.load_cropbox_v2(stack=stack, anchor_fn=None, \n",
    "                                                            prep_id='alignedWithMargin',\n",
    "                                                           return_dict=False, only_2d=True)\n",
    "            \n",
    "    for section in metadata_cache['valid_sections_all'][stack]:\n",
    "        \n",
    "        for version in ['NtbNormalized']:\n",
    "#         for version in [None, 'mask']:\n",
    "\n",
    "            in_fp = DataManager.get_image_filepath_v2(\\\n",
    "                                stack=stack, prep_id=1, section=section, version=version, resol='thumbnail')\n",
    "\n",
    "            out_fp = DataManager.get_image_filepath_v2(\\\n",
    "                                stack=stack, prep_id=5, section=section, version=version, resol='thumbnail')\n",
    "\n",
    "            create_parent_dir_if_not_exists(out_fp)\n",
    "\n",
    "            t = time.time()\n",
    "\n",
    "            im_prep1 = imread(in_fp)\n",
    "            im_prep5 = im_prep1[alignedWithMargin_ymin:alignedWithMargin_ymax+1, \n",
    "                                alignedWithMargin_xmin:alignedWithMargin_xmax+1]        \n",
    "            save_data(im_prep5, out_fp)\n",
    "            \n",
    "            sys.stderr.write('Generate prep5: %.2f seconds.\\n' % (time.time() - t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 10\n",
    "### thumbnail_NtbNormalized -> prep5_thumbnail_NtbNormalized\n",
    "align + crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 11\n",
    "### prep5_raw_NtbNormalizedAdaptiveInvertedGamma -> prep5_thumbnail_NtbNormalizedAdaptiveInvertedGamma\n",
    "rescale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 12\n",
    "### Specify prep2 (alignedBrainstemCrop) cropping box\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 13\n",
    "### prep5_raw_NtbNormalizedAdaptiveInvertedGamma -> prep2_raw_NtbNormalizedAdaptiveInvertedGamma\n",
    "crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 14\n",
    "### prep2_raw_NtbNormalizedAdaptiveInvertedGamma -> prep2_raw_NtbNormalizedAdaptiveInvertedGammaJpeg\n",
    "compress_jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
